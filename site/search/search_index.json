{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MicroKeras","text":"<p>MicroKeras is a lightweight neural network library.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install microkeras\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>import microkeras as mk\n\n# Your quick start code here\n</code></pre>"},{"location":"#api-reference","title":"API Reference","text":"<ul> <li>Activations</li> <li>Datasets</li> <li>Layers</li> <li>Losses</li> <li>Models</li> <li>Operations</li> <li>Optimizers</li> </ul>"},{"location":"api/activations/","title":"Activations","text":""},{"location":"api/activations/#relu","title":"ReLU","text":""},{"location":"api/activations/#microkeras.activations.relu.relu","title":"<code>relu(Z)</code>","text":"<p>Compute the ReLU (Rectified Linear Unit) activation function.</p> <p>ReLU returns the input for all positive values, and 0 for all non-positive values.</p> <p>Parameters: Z (numpy.ndarray): The input array.</p> <p>Returns: numpy.ndarray: An array with the same shape as Z, containing the ReLU activation values.</p> Source code in <code>microkeras/activations/relu.py</code> <pre><code>def relu(Z):\n    \"\"\"\n    Compute the ReLU (Rectified Linear Unit) activation function.\n\n    ReLU returns the input for all positive values, and 0 for all non-positive values.\n\n    Parameters:\n    Z (numpy.ndarray): The input array.\n\n    Returns:\n    numpy.ndarray: An array with the same shape as Z, containing the ReLU activation values.\n    \"\"\"\n    return np.maximum(0, Z)\n</code></pre>"},{"location":"api/activations/#relu-derivative","title":"ReLU Derivative","text":""},{"location":"api/activations/#microkeras.activations.relu_derivative.relu_derivative","title":"<code>relu_derivative(Z)</code>","text":"<p>Compute the derivative of the ReLU (Rectified Linear Unit) activation function.</p> <p>The derivative of ReLU is 1 for all positive values, and 0 for all non-positive values.</p> <p>Parameters: Z (numpy.ndarray): The input array.</p> <p>Returns: numpy.ndarray: An array with the same shape as Z, containing 1.0 where Z &gt; 0 and 0.0 elsewhere.</p> Source code in <code>microkeras/activations/relu_derivative.py</code> <pre><code>def relu_derivative(Z):\n    \"\"\"\n    Compute the derivative of the ReLU (Rectified Linear Unit) activation function.\n\n    The derivative of ReLU is 1 for all positive values, and 0 for all non-positive values.\n\n    Parameters:\n    Z (numpy.ndarray): The input array.\n\n    Returns:\n    numpy.ndarray: An array with the same shape as Z, containing 1.0 where Z &gt; 0 and 0.0 elsewhere.\n    \"\"\"\n    return np.where(Z &gt; 0, 1.0, 0.0)\n</code></pre>"},{"location":"api/activations/#sigmoid","title":"Sigmoid","text":""},{"location":"api/activations/#microkeras.activations.sigmoid.sigmoid","title":"<code>sigmoid(Z)</code>","text":"<p>Compute the sigmoid activation function.</p> <p>The sigmoid function is defined as f(x) = 1 / (1 + e^(-x)).</p> <p>Parameters: Z (numpy.ndarray): The input array.</p> <p>Returns: numpy.ndarray: An array with the same shape as Z, containing the sigmoid activation values.</p> Source code in <code>microkeras/activations/sigmoid.py</code> <pre><code>def sigmoid(Z):\n    \"\"\"\n    Compute the sigmoid activation function.\n\n    The sigmoid function is defined as f(x) = 1 / (1 + e^(-x)).\n\n    Parameters:\n    Z (numpy.ndarray): The input array.\n\n    Returns:\n    numpy.ndarray: An array with the same shape as Z, containing the sigmoid activation values.\n    \"\"\"\n    return 1 / (1 + np.exp(-Z))\n</code></pre>"},{"location":"api/activations/#sigmoid-derivative","title":"Sigmoid Derivative","text":""},{"location":"api/activations/#microkeras.activations.sigmoid_derivative.sigmoid_derivative","title":"<code>sigmoid_derivative(Z)</code>","text":"<p>Compute the derivative of the sigmoid activation function.</p> <p>The derivative of the sigmoid function is f'(x) = f(x) * (1 - f(x)), where f(x) is the sigmoid function.</p> <p>Parameters: Z (numpy.ndarray): The input array.</p> <p>Returns: numpy.ndarray: An array with the same shape as Z, containing the sigmoid derivative values.</p> Source code in <code>microkeras/activations/sigmoid_derivative.py</code> <pre><code>def sigmoid_derivative(Z):\n    \"\"\"\n    Compute the derivative of the sigmoid activation function.\n\n    The derivative of the sigmoid function is f'(x) = f(x) * (1 - f(x)),\n    where f(x) is the sigmoid function.\n\n    Parameters:\n    Z (numpy.ndarray): The input array.\n\n    Returns:\n    numpy.ndarray: An array with the same shape as Z, containing the sigmoid derivative values.\n    \"\"\"\n    activation = sigmoid(Z)\n    return activation * (1 - activation)\n</code></pre>"},{"location":"api/activations/#linear","title":"Linear","text":""},{"location":"api/activations/#microkeras.activations.linear.linear","title":"<code>linear(Z)</code>","text":"<p>Compute the linear activation function.</p> <p>The linear activation function is f(x) = x, so this function simply returns the input array unchanged.</p> <p>Parameters: Z (numpy.ndarray): The input array.</p> <p>Returns: numpy.ndarray: The input array Z, unchanged.</p> Source code in <code>microkeras/activations/linear.py</code> <pre><code>def linear(Z):\n    \"\"\"\n    Compute the linear activation function.\n\n    The linear activation function is f(x) = x, so this function\n    simply returns the input array unchanged.\n\n    Parameters:\n    Z (numpy.ndarray): The input array.\n\n    Returns:\n    numpy.ndarray: The input array Z, unchanged.\n    \"\"\"\n    return Z\n</code></pre>"},{"location":"api/activations/#linear-derivative","title":"Linear Derivative","text":""},{"location":"api/activations/#microkeras.activations.linear_derivative.linear_derivative","title":"<code>linear_derivative(Z)</code>","text":"<p>Compute the derivative of the linear activation function.</p> <p>The derivative of the linear function is always 1, so this function returns an array of ones with the same shape as the input.</p> <p>Parameters: Z (numpy.ndarray): The input array.</p> <p>Returns: numpy.ndarray: An array of ones with the same shape as Z.</p> Source code in <code>microkeras/activations/linear_derivative.py</code> <pre><code>def linear_derivative(Z):\n    \"\"\"\n    Compute the derivative of the linear activation function.\n\n    The derivative of the linear function is always 1, so this function\n    returns an array of ones with the same shape as the input.\n\n    Parameters:\n    Z (numpy.ndarray): The input array.\n\n    Returns:\n    numpy.ndarray: An array of ones with the same shape as Z.\n    \"\"\"\n    return np.ones_like(Z)\n</code></pre>"},{"location":"api/activations/#softmax","title":"Softmax","text":""},{"location":"api/activations/#microkeras.activations.softmax.softmax","title":"<code>softmax(Z)</code>","text":"<p>Compute the softmax activation function.</p> <p>The softmax function is defined as exp(x_i) / sum(exp(x_j)) for all j. This implementation uses the shifted softmax for numerical stability.</p> <p>Parameters: Z (numpy.ndarray): The input array.</p> <p>Returns: numpy.ndarray: An array with the same shape as Z, containing the softmax activation values.</p> Source code in <code>microkeras/activations/softmax.py</code> <pre><code>def softmax(Z):\n    \"\"\"\n    Compute the softmax activation function.\n\n    The softmax function is defined as exp(x_i) / sum(exp(x_j)) for all j.\n    This implementation uses the shifted softmax for numerical stability.\n\n    Parameters:\n    Z (numpy.ndarray): The input array.\n\n    Returns:\n    numpy.ndarray: An array with the same shape as Z, containing the softmax activation values.\n    \"\"\"\n    exp_shifted = np.exp(Z - np.max(Z))\n    return exp_shifted / (exp_shifted.sum(axis=0) + 1e-8)\n</code></pre>"},{"location":"api/layers/","title":"Layers","text":""},{"location":"api/layers/#dense-layer","title":"Dense Layer","text":""},{"location":"api/layers/#microkeras.layers.Dense","title":"<code>microkeras.layers.Dense</code>","text":"<p>Represents a dense (fully connected) layer in a neural network.</p> <p>This class implements a dense layer with configurable number of units and activation function.</p>"},{"location":"api/layers/#microkeras.layers.Dense--attributes","title":"Attributes","text":"<ul> <li><code>units</code> (int): The number of neurons in the layer.</li> <li><code>activation</code> (str or None): The activation function to use.</li> <li><code>input_shape</code> (int or tuple): The shape of the input to this layer.</li> </ul>"},{"location":"api/layers/#microkeras.layers.Dense--methods","title":"Methods","text":"<ul> <li><code>build(input_shape)</code>: Initialize the layer's weights and biases.</li> <li><code>copy()</code>: Create a deep copy of the layer.</li> </ul>"},{"location":"api/layers/#microkeras.layers.Dense--example","title":"Example","text":"<pre><code># Create a dense layer with 64 units and ReLU activation\nlayer = Dense(64, activation='relu', input_shape=(128,))\n\n# Build the layer\nlayer.build(input_shape=(128,))\n\n# Use the layer in a model\nx = np.random.randn(32, 128)  # Batch of 32 samples\noutput = layer.forward(x)\nprint(output.shape)  # Output: (32, 64)\n</code></pre>"},{"location":"api/layers/#microkeras.layers.Dense--notes","title":"Notes","text":"<ul> <li>The layer must be built (using <code>build()</code>) before it can be used.</li> <li>The <code>forward()</code> method is implemented separately and handles the actual computation.</li> </ul>"},{"location":"api/layers/#microkeras.layers.Dense.__init__","title":"<code>__init__(units, activation=None, input_shape=None)</code>","text":"<p>Initialize the dense layer.</p>"},{"location":"api/layers/#microkeras.layers.Dense.__init__--parameters","title":"Parameters","text":"<ul> <li><code>units</code> (int): The number of neurons in the layer.</li> <li><code>activation</code> (str or None): The activation function to use. Defaults to None.</li> <li><code>input_shape</code> (int or tuple): The shape of the input to this layer. Defaults to None.</li> </ul>"},{"location":"api/layers/#methods","title":"Methods","text":""},{"location":"api/layers/#build","title":"build","text":""},{"location":"api/layers/#microkeras.layers.dense.build.build","title":"<code>build(self, input_shape)</code>","text":"<p>Build the dense layer by initializing weights and biases.</p> <p>This method initializes the weight matrix and bias vector for the dense layer based on the input shape and the number of units in the layer.</p>"},{"location":"api/layers/#microkeras.layers.dense.build.build--parameters","title":"Parameters","text":"<ul> <li><code>input_shape</code> (int or tuple): The shape of the input to this layer. If tuple, the first element is used.</li> </ul>"},{"location":"api/layers/#microkeras.layers.dense.build.build--notes","title":"Notes","text":"<ul> <li>Weights are initialized randomly in the range [-0.5, 0.5].</li> <li>Biases are initialized randomly in the range [-0.5, 0.5].</li> </ul>"},{"location":"api/layers/#microkeras.layers.dense.build.build--example","title":"Example","text":"<pre><code>layer = Dense(64, activation='relu')\nlayer.build(input_shape=(128,))\nprint(layer.W.shape)  # Output: (64, 128)\nprint(layer.b.shape)  # Output: (64, 1)\n</code></pre>"},{"location":"api/layers/#microkeras.layers.dense.build.build--attributes-set","title":"Attributes Set","text":"<ul> <li><code>self.W</code>: Weight matrix of shape <code>(self.units, self.input_shape)</code></li> <li><code>self.b</code>: Bias vector of shape <code>(self.units, 1)</code></li> </ul> Source code in <code>microkeras/layers/dense/build.py</code> <pre><code>def build(self, input_shape):\n    \"\"\"\n    Build the dense layer by initializing weights and biases.\n\n    This method initializes the weight matrix and bias vector for the dense layer\n    based on the input shape and the number of units in the layer.\n\n    ### Parameters\n    - `input_shape` (int or tuple): The shape of the input to this layer. If tuple, the first element is used.\n\n    ### Notes\n    - **Weights** are initialized randomly in the range [-0.5, 0.5].\n    - **Biases** are initialized randomly in the range [-0.5, 0.5].\n\n    ### Example\n    ```python\n    layer = Dense(64, activation='relu')\n    layer.build(input_shape=(128,))\n    print(layer.W.shape)  # Output: (64, 128)\n    print(layer.b.shape)  # Output: (64, 1)\n    ```\n\n    ### Attributes Set\n    - `self.W`: Weight matrix of shape `(self.units, self.input_shape)`\n    - `self.b`: Bias vector of shape `(self.units, 1)`\n    \"\"\"\n    if isinstance(input_shape, tuple):\n        self.input_shape = input_shape[0]\n    else:\n        self.input_shape = input_shape\n    self.W = np.random.rand(self.units, self.input_shape) - 0.5\n    self.b = np.random.rand(self.units, 1) - 0.5\n</code></pre>"},{"location":"api/layers/#copy","title":"copy","text":""},{"location":"api/layers/#microkeras.layers.dense.copy.copy","title":"<code>copy(self)</code>","text":"<p>Create a deep copy of the dense layer.</p> <p>This method creates a new instance of the dense layer with the same configuration and performs a deep copy of the weights and biases if they have been initialized.</p>"},{"location":"api/layers/#microkeras.layers.dense.copy.copy--returns","title":"Returns","text":"<p><code>Dense</code>: A new <code>Dense</code> layer instance with copied attributes and parameters.</p>"},{"location":"api/layers/#microkeras.layers.dense.copy.copy--example","title":"Example","text":"<pre><code>original_layer = Dense(32, activation='sigmoid', input_shape=(64,))\noriginal_layer.build(input_shape=(64,))\n\ncopied_layer = original_layer.copy()\nprint(np.array_equal(original_layer.W, copied_layer.W))  # Output: True\nprint(id(original_layer.W) != id(copied_layer.W))  # Output: True\n</code></pre>"},{"location":"api/layers/#microkeras.layers.dense.copy.copy--implementation-details","title":"Implementation Details","text":"<ul> <li>Uses <code>copy.deepcopy()</code> for <code>self.W</code> and <code>self.b</code></li> <li>Creates a new instance of the same class (<code>self.__class__</code>)</li> <li>Copies all initialized attributes</li> </ul> Source code in <code>microkeras/layers/dense/copy.py</code> <pre><code>def copy(self):\n    \"\"\"\n    Create a deep copy of the dense layer.\n\n    This method creates a new instance of the dense layer with the same configuration\n    and performs a deep copy of the weights and biases if they have been initialized.\n\n    ### Returns\n    `Dense`: A new `Dense` layer instance with copied attributes and parameters.\n\n    ### Example\n    ```python\n    original_layer = Dense(32, activation='sigmoid', input_shape=(64,))\n    original_layer.build(input_shape=(64,))\n\n    copied_layer = original_layer.copy()\n    print(np.array_equal(original_layer.W, copied_layer.W))  # Output: True\n    print(id(original_layer.W) != id(copied_layer.W))  # Output: True\n    ```\n\n    ### Implementation Details\n    - Uses `copy.deepcopy()` for `self.W` and `self.b`\n    - Creates a new instance of the same class (`self.__class__`)\n    - Copies all initialized attributes\n    \"\"\"\n    new_layer = self.__class__(self.units,\n                               activation=self.activation,\n                               input_shape=self.input_shape)\n    if self.W is not None:\n        new_layer.W = cp.deepcopy(self.W)\n    if self.b is not None:\n        new_layer.b = cp.deepcopy(self.b)\n    return new_layer\n</code></pre>"},{"location":"api/layers/#initialize","title":"initialize","text":""},{"location":"api/layers/#microkeras.layers.dense.initialize.initialize","title":"<code>initialize(self, units, activation, input_shape)</code>","text":"<p>Initialize the attributes of the dense layer.</p> <p>This method sets up the basic attributes of the dense layer, including the number of units, activation function, and input shape. It also initializes placeholders for weights, biases, and other attributes that will be set during the build and training process.</p>"},{"location":"api/layers/#microkeras.layers.dense.initialize.initialize--parameters","title":"Parameters","text":"<ul> <li><code>units</code> (int): The number of neurons in the layer.</li> <li><code>activation</code> (str or None): The activation function to use.</li> <li><code>input_shape</code> (int or tuple): The shape of the input to this layer.</li> </ul>"},{"location":"api/layers/#microkeras.layers.dense.initialize.initialize--example","title":"Example","text":"<pre><code>layer = Dense(64)\nprint(layer.units)  # Output: 64\nprint(layer.activation)  # Output: None\nprint(layer.W)  # Output: None (not yet built)\n\nlayer.build(input_shape=(128,))\nprint(layer.W.shape)  # Output: (64, 128)\nprint(layer.b.shape)  # Output: (64, 1)\n</code></pre>"},{"location":"api/layers/#microkeras.layers.dense.initialize.initialize--attributes-initialized","title":"Attributes Initialized","text":"<ul> <li><code>self.units</code>: Number of neurons</li> <li><code>self.activation</code>: Activation function</li> <li><code>self.input_shape</code>: Shape of input</li> <li><code>self.W</code>, <code>self.b</code>: Weight and bias (set to <code>None</code>)</li> <li><code>self.Z</code>, <code>self.A</code>: Intermediate computations (set to <code>None</code>)</li> <li><code>self.dZ</code>, <code>self.dW</code>, <code>self.db</code>: Gradients (set to <code>None</code>)</li> </ul> Source code in <code>microkeras/layers/dense/initialize.py</code> <pre><code>def initialize(self, units, activation, input_shape):\n    \"\"\"\n    Initialize the attributes of the dense layer.\n\n    This method sets up the basic attributes of the dense layer, including the number of units,\n    activation function, and input shape. It also initializes placeholders for weights, biases,\n    and other attributes that will be set during the build and training process.\n\n    ### Parameters\n    - `units` (int): The number of neurons in the layer.\n    - `activation` (str or None): The activation function to use.\n    - `input_shape` (int or tuple): The shape of the input to this layer.\n\n    ### Example\n    ```python\n    layer = Dense(64)\n    print(layer.units)  # Output: 64\n    print(layer.activation)  # Output: None\n    print(layer.W)  # Output: None (not yet built)\n\n    layer.build(input_shape=(128,))\n    print(layer.W.shape)  # Output: (64, 128)\n    print(layer.b.shape)  # Output: (64, 1)\n    ```\n\n    ### Attributes Initialized\n    - `self.units`: Number of neurons\n    - `self.activation`: Activation function\n    - `self.input_shape`: Shape of input\n    - `self.W`, `self.b`: Weight and bias (set to `None`)\n    - `self.Z`, `self.A`: Intermediate computations (set to `None`)\n    - `self.dZ`, `self.dW`, `self.db`: Gradients (set to `None`)\n    \"\"\"\n    self.units = units\n    self.activation = activation\n    self.input_shape = input_shape\n    self.batch_size = None\n    self.W = None\n    self.b = None\n    self.Z = None\n    self.A = None\n    self.dZ = None\n    self.dW = None\n    self.db = None\n</code></pre>"}]}