{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started with MicroKeras","text":"<p>MicroKeras is a minimal implementation of the Sequential Class from the Keras deep-learning library, built from scratch using Python and NumPy. It provides a simple and intuitive API for building, training, and evaluating neural networks inspired by the Keras API.</p>"},{"location":"#installation","title":"Installation","text":"<p>To install MicroKeras, use pip:</p> <pre><code>pip install microkeras\n</code></pre>"},{"location":"#quick-start-mnist-classification-example","title":"Quick Start: MNIST Classification Example","text":"<p>Let's walk through a complete example using MicroKeras to classify handwritten digits from the MNIST dataset. This example will demonstrate how to load data, create a model, train it, make predictions, and save/load the model.</p>"},{"location":"#importing-dependencies","title":"Importing Dependencies","text":"<p>First, let's import the necessary modules:</p> <pre><code>import numpy as np\nfrom microkeras.models import Sequential\nfrom microkeras.layers import Dense\nfrom microkeras.optimizers import SGD\nfrom microkeras.datasets import mnist\n</code></pre>"},{"location":"#loading-and-preprocessing-data","title":"Loading and Preprocessing Data","text":"<p>Next, we'll load the MNIST dataset and preprocess it:</p> <pre><code># Load and preprocess MNIST data\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n# Reshape and normalize the input data\nX_train = X_train.reshape(X_train.shape[0], -1)\nX_test = X_test.reshape(X_test.shape[0], -1)\n\n# One-hot encode the labels\ny_train = np.eye(10)[y_train]\ny_test = np.eye(10)[y_test]\n\nprint(\"Data shapes:\")\nprint(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\nprint(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n</code></pre> <p>Output:</p> <pre><code>Data shapes:\nX_train: (56000, 784), y_train: (56000, 10)\nX_test: (14000, 784), y_test: (14000, 10)\n</code></pre>"},{"location":"#creating-the-model","title":"Creating the Model","text":"<p>Now, let's create our Sequential model:</p> <pre><code>model = Sequential([\n    Dense(200, activation='sigmoid', input_shape=(784,)),\n    Dense(200, activation='sigmoid'),\n    Dense(10, activation='softmax')\n])\n</code></pre> <p>This model consists of two hidden layers with sigmoid activation and an output layer with softmax activation, suitable for multi-class classification.</p>"},{"location":"#compiling-the-model","title":"Compiling the Model","text":"<p>We'll compile the model using Stochastic Gradient Descent (SGD) as the optimizer and Categorical Cross-Entropy as the loss function:</p> <pre><code>optimizer = SGD(learning_rate=0.1)\nmodel.compile(optimizer=optimizer,\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n</code></pre>"},{"location":"#training-the-model","title":"Training the Model","text":"<p>Let's train the model for 10 epochs with a batch size of 32:</p> <pre><code>history = model.fit(X_train,\n                    y_train,\n                    batch_size=16,\n                    epochs=10)\n</code></pre> <p>Output:</p> <pre><code>Epoch 1/10\nBatch 3500/3500 - Loss: 4.9410, Acc: 0.9238: : 3712it [00:37, 99.54it/s] \nEpoch 2/10\nBatch 3500/3500 - Loss: 1.9095, Acc: 0.9746: : 3712it [00:20, 180.73it/s]\nEpoch 3/10\nBatch 3500/3500 - Loss: 1.5755, Acc: 0.9844: : 3712it [00:21, 168.76it/s]\nEpoch 4/10\nBatch 3500/3500 - Loss: 1.2145, Acc: 0.9844: : 3712it [00:22, 162.42it/s]\nEpoch 5/10\nBatch 3500/3500 - Loss: 0.9108, Acc: 0.9902: : 3712it [00:23, 157.83it/s]\nEpoch 6/10\nBatch 3500/3500 - Loss: 0.7050, Acc: 0.9941: : 3712it [00:21, 175.94it/s]\nEpoch 7/10\nBatch 3500/3500 - Loss: 0.4757, Acc: 0.9980: : 3712it [00:20, 183.64it/s]\nEpoch 8/10\nBatch 3500/3500 - Loss: 0.4843, Acc: 1.0000: : 3712it [00:22, 164.84it/s]\nEpoch 9/10\nBatch 3500/3500 - Loss: 0.4095, Acc: 0.9980: : 3712it [00:22, 164.12it/s]\nEpoch 10/10\nBatch 3500/3500 - Loss: 0.3842, Acc: 0.9961: : 3712it [00:21, 176.15it/s]\n</code></pre>"},{"location":"#plotting-training-history","title":"Plotting Training History","text":"<p>We can plot the training history using Matplotlib:</p> <pre><code># Plot training history\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(history['accuracy'], label='Training Accuracy')\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history['loss'], label='Training Loss')\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Output:</p> <p></p>"},{"location":"#evaluating-the-model","title":"Evaluating the Model","text":"<p>After training, we can evaluate the model on the test set:</p> <pre><code>test_loss, test_accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test accuracy: {test_accuracy:.4f}\")\n</code></pre> <p>Output:</p> <pre><code>Test accuracy: 0.9671\n</code></pre>"},{"location":"#making-predictions","title":"Making Predictions","text":"<p>Let's make predictions for the first 5 test samples:</p> <pre><code>predictions = model.predict(X_test[:5])\nprint(\"Predictions for the first 5 test samples:\")\nprint(np.argmax(predictions, axis=1))\nprint(\"Actual labels:\")\nprint(np.argmax(y_test[:5], axis=1))\n\n# Visualize the predictions\nplt.figure(figsize=(12, 4))\nfor i in range(5):\n    plt.subplot(1, 5, i+1)\n    plt.imshow(X_test[i].reshape(28, 28), cmap='gray')\n    plt.title(f\"Pred: {np.argmax(predictions[i])}\\nTrue: {np.argmax(y_test[i])}\")\n    plt.axis('off')\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Output:</p> <pre><code>Predictions for the first 5 test samples:\n[8 4 8 7 7]\nActual labels:\n[8 4 8 7 7]\n</code></pre> <p></p>"},{"location":"#saving-and-loading-the-model","title":"Saving and Loading the Model","text":"<p>MicroKeras allows you to save and load models:</p> <pre><code># Save the model\nmodel.save('mnist_model.json')\n\n# Load the model\nloaded_model = Sequential.load('mnist_model.json')\n\n# Compile the loaded model\nloaded_model.compile(optimizer=optimizer,\n                     loss='categorical_crossentropy',\n                     metrics=['accuracy'])\n\n# Evaluate the loaded model\nloaded_test_loss, loaded_test_accuracy = loaded_model.evaluate(X_test, y_test)\nprint(f\"Loaded model test accuracy: {loaded_test_accuracy:.4f}\")\n</code></pre> <p>Output:</p> <pre><code>Loaded model test accuracy: 0.9671\n</code></pre>"},{"location":"#viewing-training-history","title":"Viewing Training History","text":"<p>Finally, let's print out the training history:</p> <pre><code>print(\"\\nTraining History:\")\nprint(\"Epoch\\tAccuracy\\tLoss\")\nfor epoch, (accuracy, loss) in enumerate(zip(history['accuracy'], history['loss']), 1):\n    print(f\"{epoch}\\t{accuracy:.4f}\\t{loss:.4f}\")\n</code></pre> <p>Output:</p> <pre><code>Training History:\nEpoch   Accuracy    Loss\n1   0.9235  14096.9166\n2   0.9467  9728.8764\n3   0.9592  7629.1049\n4   0.9683  6160.9156\n5   0.9730  5208.7316\n6   0.9761  4604.9854\n7   0.9803  3969.1862\n8   0.9828  3443.3674\n9   0.9831  3310.2258\n10  0.9851  3007.7300\n</code></pre>"},{"location":"#features","title":"Features","text":"<p>MicroKeras offers the following features:</p> <ul> <li>Sequential model API</li> <li>Dense (fully connected) layers</li> <li>Various activation functions (ReLU, Sigmoid, Softmax, Linear)</li> <li>Loss functions (Mean Squared Error, Categorical Cross-Entropy)</li> <li>Optimizers (Stochastic Gradient Descent)</li> <li>Dataset loaders (MNIST, California Housing)</li> <li>Model saving and loading</li> </ul>"},{"location":"#additional-examples","title":"Additional Examples","text":""},{"location":"#california-housing-regression","title":"California Housing Regression","text":"<p>For an example of using MicroKeras for regression tasks, see our California Housing Regression Example.</p>"},{"location":"Api_Reference/Activations/Linear/","title":"Linear","text":""},{"location":"Api_Reference/Activations/Linear/#microkeras.activations.linear-functions","title":"Functions","text":""},{"location":"Api_Reference/Activations/Linear/#microkeras.activations.linear.linear","title":"<code>linear(Z)</code>","text":"<p>Compute the linear activation function.</p> <p>Parameters:</p> Name Type Description Default <code>Z</code> <code>ndarray</code> <p>The input array.</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: The input array Z, unchanged.</p> Example <pre><code>Z = np.array([[1, 2], [3, 4]])\nA = linear(Z)\nprint(A)\n</code></pre> Source code in <code>microkeras/activations/linear.py</code> <pre><code>def linear(Z):\n    \"\"\"\n    Compute the linear activation function.\n\n    Args:\n        Z (numpy.ndarray): The input array.\n\n    Returns:\n        numpy.ndarray: The input array Z, unchanged.\n\n    Example:\n        ```python\n        Z = np.array([[1, 2], [3, 4]])\n        A = linear(Z)\n        print(A)\n        ```\n    \"\"\"\n    return Z\n</code></pre>"},{"location":"Api_Reference/Activations/Linear_Derivative/","title":"Linear Derivative","text":""},{"location":"Api_Reference/Activations/Linear_Derivative/#microkeras.activations.linear_derivative-functions","title":"Functions","text":""},{"location":"Api_Reference/Activations/Linear_Derivative/#microkeras.activations.linear_derivative.linear_derivative","title":"<code>linear_derivative(Z)</code>","text":"<p>Compute the derivative of the linear activation function.</p> <p>Parameters:</p> Name Type Description Default <code>Z</code> <code>ndarray</code> <p>The input array.</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: An array of ones with the same shape as Z.</p> Example <pre><code>Z = np.array([[1, 2], [3, 4]])\ndZ = linear_derivative(Z)\nprint(dZ)\n</code></pre> Source code in <code>microkeras/activations/linear_derivative.py</code> <pre><code>def linear_derivative(Z):\n    \"\"\"\n    Compute the derivative of the linear activation function.\n\n    Args:\n        Z (numpy.ndarray): The input array.\n\n    Returns:\n        numpy.ndarray: An array of ones with the same shape as Z.\n\n    Example:\n        ```python\n        Z = np.array([[1, 2], [3, 4]])\n        dZ = linear_derivative(Z)\n        print(dZ)\n        ```\n    \"\"\"\n    return np.ones_like(Z)\n</code></pre>"},{"location":"Api_Reference/Activations/ReLU/","title":"ReLU","text":""},{"location":"Api_Reference/Activations/ReLU/#microkeras.activations.relu-functions","title":"Functions","text":""},{"location":"Api_Reference/Activations/ReLU/#microkeras.activations.relu.relu","title":"<code>relu(Z)</code>","text":"<p>Compute the ReLU (Rectified Linear Unit) activation function.</p> <p>Parameters:</p> Name Type Description Default <code>Z</code> <code>ndarray</code> <p>The input array.</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: An array with the same shape as Z, containing the ReLU activation values.</p> Example <pre><code>Z = np.array([[-1, 0], [1, 2]])\nA = relu(Z)\nprint(A)\n</code></pre> Source code in <code>microkeras/activations/relu.py</code> <pre><code>def relu(Z):\n    \"\"\"\n    Compute the ReLU (Rectified Linear Unit) activation function.\n\n    Args:\n        Z (numpy.ndarray): The input array.\n\n    Returns:\n        numpy.ndarray: An array with the same shape as Z, containing the ReLU activation values.\n\n    Example:\n        ```python\n        Z = np.array([[-1, 0], [1, 2]])\n        A = relu(Z)\n        print(A)\n        ```\n    \"\"\"\n    return np.maximum(0, Z)\n</code></pre>"},{"location":"Api_Reference/Activations/ReLU_Derivative/","title":"ReLU Derivative","text":""},{"location":"Api_Reference/Activations/ReLU_Derivative/#microkeras.activations.relu_derivative-functions","title":"Functions","text":""},{"location":"Api_Reference/Activations/ReLU_Derivative/#microkeras.activations.relu_derivative.relu_derivative","title":"<code>relu_derivative(Z)</code>","text":"<p>Compute the derivative of the ReLU (Rectified Linear Unit) activation function.</p> <p>Parameters:</p> Name Type Description Default <code>Z</code> <code>ndarray</code> <p>The input array.</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: An array with the same shape as Z, containing 1.0 where Z &gt; 0 and 0.0 elsewhere.</p> Example <pre><code>Z = np.array([[-1, 0], [1, 2]])\ndZ = relu_derivative(Z)\nprint(dZ)\n</code></pre> Source code in <code>microkeras/activations/relu_derivative.py</code> <pre><code>def relu_derivative(Z):\n    \"\"\"\n    Compute the derivative of the ReLU (Rectified Linear Unit) activation function.\n\n    Args:\n        Z (numpy.ndarray): The input array.\n\n    Returns:\n        numpy.ndarray: An array with the same shape as Z, containing 1.0 where Z &gt; 0 and 0.0 elsewhere.\n\n    Example:\n        ```python\n        Z = np.array([[-1, 0], [1, 2]])\n        dZ = relu_derivative(Z)\n        print(dZ)\n        ```\n    \"\"\"\n    return np.where(Z &gt; 0, 1.0, 0.0)\n</code></pre>"},{"location":"Api_Reference/Activations/Sigmoid/","title":"Sigmoid","text":""},{"location":"Api_Reference/Activations/Sigmoid/#microkeras.activations.sigmoid-functions","title":"Functions","text":""},{"location":"Api_Reference/Activations/Sigmoid/#microkeras.activations.sigmoid.sigmoid","title":"<code>sigmoid(Z)</code>","text":"<p>Compute the sigmoid activation function.</p> <p>Parameters:</p> Name Type Description Default <code>Z</code> <code>ndarray</code> <p>The input array.</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: An array with the same shape as Z, containing the sigmoid activation values.</p> Example <pre><code>Z = np.array([[-1, 0], [1, 2]])\nA = sigmoid(Z)\nprint(A)\n</code></pre> Source code in <code>microkeras/activations/sigmoid.py</code> <pre><code>def sigmoid(Z):\n    \"\"\"\n    Compute the sigmoid activation function.\n\n    Args:\n        Z (numpy.ndarray): The input array.\n\n    Returns:\n        numpy.ndarray: An array with the same shape as Z, containing the sigmoid activation values.\n\n    Example:\n        ```python\n        Z = np.array([[-1, 0], [1, 2]])\n        A = sigmoid(Z)\n        print(A)\n        ```\n    \"\"\"\n    return 1 / (1 + np.exp(-Z))\n</code></pre>"},{"location":"Api_Reference/Activations/Sigmoid_Derivative/","title":"Sigmoid Derivative","text":""},{"location":"Api_Reference/Activations/Sigmoid_Derivative/#microkeras.activations.sigmoid_derivative-functions","title":"Functions","text":""},{"location":"Api_Reference/Activations/Sigmoid_Derivative/#microkeras.activations.sigmoid_derivative.sigmoid_derivative","title":"<code>sigmoid_derivative(Z)</code>","text":"<p>Compute the derivative of the sigmoid activation function.</p> <p>Parameters:</p> Name Type Description Default <code>Z</code> <code>ndarray</code> <p>The input array.</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: An array with the same shape as Z, containing the sigmoid derivative values.</p> Example <pre><code>Z = np.array([[-1, 0], [1, 2]])\ndZ = sigmoid_derivative(Z)\nprint(dZ)\n</code></pre> Source code in <code>microkeras/activations/sigmoid_derivative.py</code> <pre><code>def sigmoid_derivative(Z):\n    \"\"\"\n    Compute the derivative of the sigmoid activation function.\n\n    Args:\n        Z (numpy.ndarray): The input array.\n\n    Returns:\n        numpy.ndarray: An array with the same shape as Z, containing the sigmoid derivative values.\n\n    Example:\n        ```python\n        Z = np.array([[-1, 0], [1, 2]])\n        dZ = sigmoid_derivative(Z)\n        print(dZ)\n        ```\n    \"\"\"\n    activation = sigmoid(Z)\n    return activation * (1 - activation)\n</code></pre>"},{"location":"Api_Reference/Activations/Softmax/","title":"Softmax","text":""},{"location":"Api_Reference/Activations/Softmax/#microkeras.activations.softmax-functions","title":"Functions","text":""},{"location":"Api_Reference/Activations/Softmax/#microkeras.activations.softmax.softmax","title":"<code>softmax(Z)</code>","text":"<p>Compute the softmax activation function.</p> <p>Parameters:</p> Name Type Description Default <code>Z</code> <code>ndarray</code> <p>The input array.</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: An array with the same shape as Z, containing the softmax activation values.</p> Example <pre><code>Z = np.array([[1, 2], [3, 4]])\nA = softmax(Z)\nprint(A)  # Output: [[0.119203 0.119203] [0.880797 0.880797]]\n</code></pre> Source code in <code>microkeras/activations/softmax.py</code> <pre><code>def softmax(Z):\n    \"\"\"\n    Compute the softmax activation function.\n\n    Args:\n        Z (numpy.ndarray): The input array.\n\n    Returns:\n        numpy.ndarray: An array with the same shape as Z, containing the softmax activation values.\n\n    Example:\n        ```python\n        Z = np.array([[1, 2], [3, 4]])\n        A = softmax(Z)\n        print(A)  # Output: [[0.119203 0.119203] [0.880797 0.880797]]\n        ```\n    \"\"\"\n    exp_shifted = np.exp(Z - np.max(Z))\n    return exp_shifted / (exp_shifted.sum(axis=0) + 1e-8)\n</code></pre>"},{"location":"Api_Reference/Layers/Dense/Build_Method/","title":"Build Method","text":""},{"location":"Api_Reference/Layers/Dense/Build_Method/#microkeras.layers.dense.build-functions","title":"Functions","text":""},{"location":"Api_Reference/Layers/Dense/Build_Method/#microkeras.layers.dense.build.build","title":"<code>build(self, input_shape)</code>","text":"<p>Build the dense layer by initializing weights and biases.</p> <p>Parameters:</p> Name Type Description Default <code>input_shape</code> <code>int or tuple</code> <p>The shape of the input to this layer. If tuple, the first element is used.</p> required Example <pre><code>layer = Dense(64, activation='relu')\nlayer.build(input_shape=(128,))\nprint(layer.W.shape)  # Output: (64, 128)\nprint(layer.b.shape)  # Output: (64, 1)\n</code></pre> Source code in <code>microkeras/layers/dense/build.py</code> <pre><code>def build(self, input_shape):\n    \"\"\"\n    Build the dense layer by initializing weights and biases.\n\n    Args:\n        input_shape (int or tuple): The shape of the input to this layer. If tuple, the first element is used.\n\n    Example:\n        ```python\n        layer = Dense(64, activation='relu')\n        layer.build(input_shape=(128,))\n        print(layer.W.shape)  # Output: (64, 128)\n        print(layer.b.shape)  # Output: (64, 1)\n        ```\n    \"\"\"\n    if isinstance(input_shape, tuple):\n        self.input_shape = input_shape[0]\n    else:\n        self.input_shape = input_shape\n    self.W = np.random.rand(self.units, self.input_shape) - 0.5\n    self.b = np.random.rand(self.units, 1) - 0.5\n</code></pre>"},{"location":"Api_Reference/Layers/Dense/Copy_Method/","title":"Copy Method","text":""},{"location":"Api_Reference/Layers/Dense/Copy_Method/#microkeras.layers.dense.copy-functions","title":"Functions","text":""},{"location":"Api_Reference/Layers/Dense/Copy_Method/#microkeras.layers.dense.copy.copy","title":"<code>copy(self)</code>","text":"<p>Create a deep copy of the dense layer.</p> <p>Returns:</p> Name Type Description <code>Dense</code> <p>A new Dense layer instance with copied attributes and parameters.</p> Example <pre><code>original_layer = Dense(32, activation='sigmoid', input_shape=(64,))\noriginal_layer.build(input_shape=(64,))\n\ncopied_layer = original_layer.copy()\nprint(np.array_equal(original_layer.W, copied_layer.W))  # Output: True\nprint(id(original_layer.W) != id(copied_layer.W))  # Output: True\n</code></pre> Source code in <code>microkeras/layers/dense/copy.py</code> <pre><code>def copy(self):\n    \"\"\"\n    Create a deep copy of the dense layer.\n\n    Returns:\n        Dense: A new Dense layer instance with copied attributes and parameters.\n\n    Example:\n        ```python\n        original_layer = Dense(32, activation='sigmoid', input_shape=(64,))\n        original_layer.build(input_shape=(64,))\n\n        copied_layer = original_layer.copy()\n        print(np.array_equal(original_layer.W, copied_layer.W))  # Output: True\n        print(id(original_layer.W) != id(copied_layer.W))  # Output: True\n        ```\n    \"\"\"\n    new_layer = self.__class__(self.units,\n                               activation=self.activation,\n                               input_shape=self.input_shape)\n    if self.W is not None:\n        new_layer.W = cp.deepcopy(self.W)\n    if self.b is not None:\n        new_layer.b = cp.deepcopy(self.b)\n    return new_layer\n</code></pre>"},{"location":"Api_Reference/Layers/Dense/Dense_Class/","title":"Dense Class","text":"<p>Represents a dense (fully connected) layer in a neural network.</p> <p>Attributes:</p> Name Type Description <code>units</code> <code>int</code> <p>The number of neurons in the layer.</p> <code>activation</code> <code>str or None</code> <p>The activation function to use.</p> <code>input_shape</code> <code>int or tuple</code> <p>The shape of the input to this layer.</p> Example <pre><code># Create a dense layer with 64 units and ReLU activation\nlayer = Dense(64, activation='relu', input_shape=(128,))\n\n# Build the layer\nlayer.build(input_shape=(128,))\n\n# Use the layer in a model\nx = np.random.randn(32, 128)  # Batch of 32 samples\noutput = layer.forward(x)\nprint(output.shape)  # Output: (32, 64)\n</code></pre> Source code in <code>microkeras/layers/dense/dense.py</code> <pre><code>class Dense:\n    \"\"\"\n    Represents a dense (fully connected) layer in a neural network.\n\n    Attributes:\n        units (int): The number of neurons in the layer.\n        activation (str or None): The activation function to use.\n        input_shape (int or tuple): The shape of the input to this layer.\n\n    Example:\n        ```python\n        # Create a dense layer with 64 units and ReLU activation\n        layer = Dense(64, activation='relu', input_shape=(128,))\n\n        # Build the layer\n        layer.build(input_shape=(128,))\n\n        # Use the layer in a model\n        x = np.random.randn(32, 128)  # Batch of 32 samples\n        output = layer.forward(x)\n        print(output.shape)  # Output: (32, 64)\n        ```\n    \"\"\"\n    def __init__(self, units, activation=None, input_shape=None):\n        \"\"\"\n        Initialize the dense layer.\n\n        Args:\n            units (int): The number of neurons in the layer.\n            activation (str or None): The activation function to use. Default is None.\n            input_shape (int or tuple): The shape of the input to this layer. Default is None.\n        \"\"\"\n        initialize(self, units, activation, input_shape)\n\n    build = build\n    copy = copy\n</code></pre>"},{"location":"Api_Reference/Layers/Dense/Dense_Class/#microkeras.layers.Dense-functions","title":"Functions","text":""},{"location":"Api_Reference/Layers/Dense/Dense_Class/#microkeras.layers.Dense.__init__","title":"<code>__init__(units, activation=None, input_shape=None)</code>","text":"<p>Initialize the dense layer.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>int</code> <p>The number of neurons in the layer.</p> required <code>activation</code> <code>str or None</code> <p>The activation function to use. Default is None.</p> <code>None</code> <code>input_shape</code> <code>int or tuple</code> <p>The shape of the input to this layer. Default is None.</p> <code>None</code> Source code in <code>microkeras/layers/dense/dense.py</code> <pre><code>def __init__(self, units, activation=None, input_shape=None):\n    \"\"\"\n    Initialize the dense layer.\n\n    Args:\n        units (int): The number of neurons in the layer.\n        activation (str or None): The activation function to use. Default is None.\n        input_shape (int or tuple): The shape of the input to this layer. Default is None.\n    \"\"\"\n    initialize(self, units, activation, input_shape)\n</code></pre>"},{"location":"Api_Reference/Layers/Dense/Initialize_Method/","title":"Initialize Method","text":""},{"location":"Api_Reference/Layers/Dense/Initialize_Method/#microkeras.layers.dense.initialize-functions","title":"Functions","text":""},{"location":"Api_Reference/Layers/Dense/Initialize_Method/#microkeras.layers.dense.initialize.initialize","title":"<code>initialize(self, units, activation, input_shape)</code>","text":"<p>Initialize the attributes of the dense layer.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>int</code> <p>The number of neurons in the layer.</p> required <code>activation</code> <code>str or None</code> <p>The activation function to use.</p> required <code>input_shape</code> <code>int or tuple</code> <p>The shape of the input to this layer.</p> required Example <pre><code>layer = Dense(64)\nprint(layer.units)  # Output: 64\nprint(layer.activation)  # Output: None\nprint(layer.W)  # Output: None (not yet built)\n\nlayer.build(input_shape=(128,))\nprint(layer.W.shape)  # Output: (64, 128)\nprint(layer.b.shape)  # Output: (64, 1)\n</code></pre> Source code in <code>microkeras/layers/dense/initialize.py</code> <pre><code>def initialize(self, units, activation, input_shape):\n    \"\"\"\n    Initialize the attributes of the dense layer.\n\n    Args:\n        units (int): The number of neurons in the layer.\n        activation (str or None): The activation function to use.\n        input_shape (int or tuple): The shape of the input to this layer.\n\n    Example:\n        ```python\n        layer = Dense(64)\n        print(layer.units)  # Output: 64\n        print(layer.activation)  # Output: None\n        print(layer.W)  # Output: None (not yet built)\n\n        layer.build(input_shape=(128,))\n        print(layer.W.shape)  # Output: (64, 128)\n        print(layer.b.shape)  # Output: (64, 1)\n        ```\n    \"\"\"\n    self.units = units\n    self.activation = activation\n    self.input_shape = input_shape\n    self.batch_size = None\n    self.W = None\n    self.b = None\n    self.Z = None\n    self.A = None\n    self.dZ = None\n    self.dW = None\n    self.db = None\n</code></pre>"},{"location":"Api_Reference/Losses/Categorical_Crossentropy/","title":"Categorical Crossentropy","text":""},{"location":"Api_Reference/Losses/Categorical_Crossentropy/#microkeras.losses.categorical_crossentropy-functions","title":"Functions","text":""},{"location":"Api_Reference/Losses/Categorical_Crossentropy/#microkeras.losses.categorical_crossentropy.categorical_crossentropy","title":"<code>categorical_crossentropy(Y, Y_hat)</code>","text":"<p>Calculate the categorical cross-entropy loss.</p> <p>Parameters:</p> Name Type Description Default <code>Y</code> <code>ndarray</code> <p>True labels in one-hot encoded format.</p> required <code>Y_hat</code> <code>ndarray</code> <p>Predicted probabilities for each class.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The categorical cross-entropy loss.</p> Example <pre><code>Y = np.array([[1, 0], [0, 1]])\nY_hat = np.array([[0.9, 0.1], [0.2, 0.8]])\nloss = categorical_crossentropy(Y, Y_hat)\nprint(loss)\n</code></pre> Note <p>A small epsilon (1e-8) is added to the log to avoid numerical instability when Y_hat contains zero probabilities.</p> Source code in <code>microkeras/losses/categorical_crossentropy.py</code> <pre><code>def categorical_crossentropy(Y, Y_hat):\n    \"\"\"\n    Calculate the categorical cross-entropy loss.\n\n    Args:\n        Y (numpy.ndarray): True labels in one-hot encoded format.\n        Y_hat (numpy.ndarray): Predicted probabilities for each class.\n\n    Returns:\n        float: The categorical cross-entropy loss.\n\n    Example:\n        ```python\n        Y = np.array([[1, 0], [0, 1]])\n        Y_hat = np.array([[0.9, 0.1], [0.2, 0.8]])\n        loss = categorical_crossentropy(Y, Y_hat)\n        print(loss)\n        ```\n\n    Note:\n        A small epsilon (1e-8) is added to the log to avoid numerical instability\n        when Y_hat contains zero probabilities.\n    \"\"\"\n    return -np.sum(Y * np.log(Y_hat + 1e-8))\n</code></pre>"},{"location":"Api_Reference/Losses/Mean_Squared_Error/","title":"Mean Squared Error","text":""},{"location":"Api_Reference/Losses/Mean_Squared_Error/#microkeras.losses.mean_squared_error-functions","title":"Functions","text":""},{"location":"Api_Reference/Losses/Mean_Squared_Error/#microkeras.losses.mean_squared_error.mean_squared_error","title":"<code>mean_squared_error(Y, Y_hat)</code>","text":"<p>Calculate the mean squared error loss.</p> <p>Parameters:</p> Name Type Description Default <code>Y</code> <code>ndarray</code> <p>True values.</p> required <code>Y_hat</code> <code>ndarray</code> <p>Predicted values.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The mean squared error between Y and Y_hat.</p> Example <pre><code>Y = np.array([[1, 2], [3, 4]])\nY_hat = np.array([[1.1, 2.1], [2.9, 4.1]])\nmse = mean_squared_error(Y, Y_hat)\nprint(mse)\n</code></pre> Note <p>This function computes the average squared difference between the true values and the predicted values.</p> Source code in <code>microkeras/losses/mean_squared_error.py</code> <pre><code>def mean_squared_error(Y, Y_hat):\n    \"\"\"\n    Calculate the mean squared error loss.\n\n    Args:\n        Y (numpy.ndarray): True values.\n        Y_hat (numpy.ndarray): Predicted values.\n\n    Returns:\n        float: The mean squared error between Y and Y_hat.\n\n    Example:\n        ```python\n        Y = np.array([[1, 2], [3, 4]])\n        Y_hat = np.array([[1.1, 2.1], [2.9, 4.1]])\n        mse = mean_squared_error(Y, Y_hat)\n        print(mse)\n        ```\n\n    Note:\n        This function computes the average squared difference\n        between the true values and the predicted values.\n    \"\"\"\n    return np.mean((Y - Y_hat) ** 2)\n</code></pre>"},{"location":"Api_Reference/Losses/Mean_Squared_Error_Derivative/","title":"Mean Squared Error Derivative","text":""},{"location":"Api_Reference/Losses/Mean_Squared_Error_Derivative/#microkeras.losses.mean_squared_error_derivative-functions","title":"Functions","text":""},{"location":"Api_Reference/Losses/Mean_Squared_Error_Derivative/#microkeras.losses.mean_squared_error_derivative.mean_squared_error_derivative","title":"<code>mean_squared_error_derivative(Y, Y_hat)</code>","text":"<p>Calculate the derivative of the mean squared error loss.</p> <p>Parameters:</p> Name Type Description Default <code>Y</code> <code>ndarray</code> <p>True values.</p> required <code>Y_hat</code> <code>ndarray</code> <p>Predicted values.</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: The gradient of the mean squared error.</p> Example <pre><code>Y = np.array([[1, 2], [3, 4]])\nY_hat = np.array([[1.1, 2.1], [2.9, 4.1]])\ngradient = mean_squared_error_derivative(Y, Y_hat)\nprint(gradient)\n</code></pre> Note <p>The result is divided by the number of samples (Y.shape[1]) to get the average gradient across all samples.</p> Source code in <code>microkeras/losses/mean_squared_error_derivative.py</code> <pre><code>def mean_squared_error_derivative(Y, Y_hat):\n    \"\"\"\n    Calculate the derivative of the mean squared error loss.\n\n    Args:\n        Y (numpy.ndarray): True values.\n        Y_hat (numpy.ndarray): Predicted values.\n\n    Returns:\n        numpy.ndarray: The gradient of the mean squared error.\n\n    Example:\n        ```python\n        Y = np.array([[1, 2], [3, 4]])\n        Y_hat = np.array([[1.1, 2.1], [2.9, 4.1]])\n        gradient = mean_squared_error_derivative(Y, Y_hat)\n        print(gradient)\n        ```\n\n    Note:\n        The result is divided by the number of samples (Y.shape[1])\n        to get the average gradient across all samples.\n    \"\"\"\n    return 2 * (Y_hat - Y) / Y.shape[1]\n</code></pre>"},{"location":"Api_Reference/Models/Sequential/Add_Method/","title":"Add Method","text":""},{"location":"Api_Reference/Models/Sequential/Add_Method/#microkeras.models.sequential.add-functions","title":"Functions","text":""},{"location":"Api_Reference/Models/Sequential/Add_Method/#microkeras.models.sequential.add.add","title":"<code>add(self, layer)</code>","text":"<p>Add a layer to the Sequential model.</p> <p>If the layer doesn't have an input_shape and there are existing layers, the input shape is inferred from the previous layer's units.</p> <p>Parameters:</p> Name Type Description Default <code>layer</code> <code>Layer</code> <p>The layer to be added to the model.</p> required Example <pre><code>model = Sequential([])\nmodel.add(Dense(64, activation='relu', input_shape=(784,)))\nmodel.add(Dense(10, activation='softmax'))\n</code></pre> Source code in <code>microkeras/models/sequential/add.py</code> <pre><code>def add(self, layer):\n    \"\"\"\n    Add a layer to the Sequential model.\n\n    If the layer doesn't have an input_shape and there are existing layers,\n    the input shape is inferred from the previous layer's units.\n\n    Args:\n        layer (Layer): The layer to be added to the model.\n\n    Example:\n        ```python\n        model = Sequential([])\n        model.add(Dense(64, activation='relu', input_shape=(784,)))\n        model.add(Dense(10, activation='softmax'))\n        ```\n    \"\"\"\n    if self.layers and layer.input_shape is None:\n        prev_layer = self.layers[-1]\n        layer.input_shape = prev_layer.units\n    self.layers.append(layer)\n</code></pre>"},{"location":"Api_Reference/Models/Sequential/Build_Method/","title":"Build Method","text":""},{"location":"Api_Reference/Models/Sequential/Build_Method/#microkeras.models.sequential.build-functions","title":"Functions","text":""},{"location":"Api_Reference/Models/Sequential/Build_Method/#microkeras.models.sequential.build.build","title":"<code>build(self)</code>","text":"<p>This method calls the build method of each layer in the model, initializing their weights and biases.</p> Source code in <code>microkeras/models/sequential/build.py</code> <pre><code>def build(self):\n    \"\"\"\n    This method calls the build method of each layer in the model,\n    initializing their weights and biases.\n    \"\"\"\n    for layer in self.layers:\n        layer.build(layer.input_shape)\n</code></pre>"},{"location":"Api_Reference/Models/Sequential/Initialize_Method/","title":"Initialize Method","text":""},{"location":"Api_Reference/Models/Sequential/Initialize_Method/#microkeras.models.sequential.initialize-functions","title":"Functions","text":""},{"location":"Api_Reference/Models/Sequential/Initialize_Method/#microkeras.models.sequential.initialize.initialize","title":"<code>initialize(self, layers)</code>","text":"<p>Initialize the Sequential model with given layers.</p> <p>Parameters:</p> Name Type Description Default <code>layers</code> <code>list</code> <p>List of Layer instances to add to the model.</p> required Note <p>This method is called internally by the Sequential constructor.</p> Source code in <code>microkeras/models/sequential/initialize.py</code> <pre><code>def initialize(self, layers):\n    \"\"\"\n    Initialize the Sequential model with given layers.\n\n    Args:\n        layers (list): List of Layer instances to add to the model.\n\n    Note:\n        This method is called internally by the Sequential constructor.\n    \"\"\"\n    self.layers = []\n    for layer in layers:\n        self.add(layer)\n    self.build()\n</code></pre>"},{"location":"Api_Reference/Models/Sequential/Load_Method/","title":"Load Method","text":""},{"location":"Api_Reference/Models/Sequential/Load_Method/#microkeras.models.sequential.load-functions","title":"Functions","text":""},{"location":"Api_Reference/Models/Sequential/Load_Method/#microkeras.models.sequential.load.load","title":"<code>load(cls, filename)</code>","text":"<p>Load a Sequential model from a file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the file containing the saved model.</p> required <p>Returns:</p> Name Type Description <code>Sequential</code> <p>A new Sequential model instance loaded from the file.</p> Example <pre><code>loaded_model = Sequential.load('my_model.json')\n</code></pre> Source code in <code>microkeras/models/sequential/load.py</code> <pre><code>def load(cls, filename):\n    \"\"\"\n    Load a Sequential model from a file.\n\n    Args:\n        filename (str): Path to the file containing the saved model.\n\n    Returns:\n        Sequential: A new Sequential model instance loaded from the file.\n\n    Example:\n        ```python\n        loaded_model = Sequential.load('my_model.json')\n        ```\n    \"\"\"\n    from microkeras.layers import Dense\n\n    with open(filename, 'r') as f:\n        model_data = json.load(f)\n\n    new_model = cls([])\n\n    for layer_data in model_data['layers']:\n        layer = Dense(\n            units=layer_data['units'],\n            activation=layer_data['activation'],\n            input_shape=layer_data['input_shape']\n        )\n        layer.W = np.array(layer_data['weights'])\n        layer.b = np.array(layer_data['biases'])\n        new_model.add(layer)\n\n    return new_model\n</code></pre>"},{"location":"Api_Reference/Models/Sequential/Predict_Method/","title":"Predict Method","text":""},{"location":"Api_Reference/Models/Sequential/Predict_Method/#microkeras.models.sequential.predict-functions","title":"Functions","text":""},{"location":"Api_Reference/Models/Sequential/Predict_Method/#microkeras.models.sequential.predict.predict","title":"<code>predict(self, X)</code>","text":"<p>Generate output predictions for the input samples.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Input data.</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: Predictions for the input data.</p> Example <pre><code>predictions = model.predict(X_test)\n</code></pre> Source code in <code>microkeras/models/sequential/predict.py</code> <pre><code>def predict(self, X):\n    \"\"\"\n    Generate output predictions for the input samples.\n\n    Args:\n        X (numpy.ndarray): Input data.\n\n    Returns:\n        numpy.ndarray: Predictions for the input data.\n\n    Example:\n        ```python\n        predictions = model.predict(X_test)\n        ```\n    \"\"\"\n    # Transpose the input data\n    X = X.T\n\n    # Forward pass\n    forward(self, X)\n\n    # Return the output of the last layer\n    predictions = self.layers[-1].A\n\n    # Transpose the predictions to match the expected output shape\n    return predictions.T\n</code></pre>"},{"location":"Api_Reference/Models/Sequential/Save_Method/","title":"Save Method","text":""},{"location":"Api_Reference/Models/Sequential/Save_Method/#microkeras.models.sequential.save-functions","title":"Functions","text":""},{"location":"Api_Reference/Models/Sequential/Save_Method/#microkeras.models.sequential.save.save","title":"<code>save(self, filename)</code>","text":"<p>Save the Sequential model to a file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path where the model should be saved.</p> required Example <pre><code>model.save('my_model.json')\n</code></pre> Source code in <code>microkeras/models/sequential/save.py</code> <pre><code>def save(self, filename):\n    \"\"\"\n    Save the Sequential model to a file.\n\n    Args:\n        filename (str): Path where the model should be saved.\n\n    Example:\n        ```python\n        model.save('my_model.json')\n        ```\n    \"\"\"\n    model_data = {\n        'layers': []\n    }\n    for layer in self.layers:\n        layer_data = {\n            'type': 'Dense',\n            'units': layer.units,\n            'activation': layer.activation,\n            'input_shape': layer.input_shape,\n            'weights': layer.W.tolist(),\n            'biases': layer.b.tolist()\n        }\n        model_data['layers'].append(layer_data)\n\n    with open(filename, 'w') as f:\n        json.dump(model_data, f)\n</code></pre>"},{"location":"Api_Reference/Models/Sequential/Sequential_Class/","title":"Sequential Class","text":"<p>The Sequential model is a linear stack of layers, useful for straightforward neural network architectures. Layers are added via the constructor or through the <code>.add()</code> method.</p> <p>Attributes:</p> Name Type Description <code>layers</code> <code>list</code> <p>List of Layer instances in the model.</p> Example <pre><code>model = Sequential([\n    Dense(64, activation='relu', input_shape=(784,)),\n    Dense(10, activation='softmax')\n])\n</code></pre> Source code in <code>microkeras/models/sequential/sequential.py</code> <pre><code>class Sequential:\n    \"\"\"\n    The Sequential model is a linear stack of layers, useful for\n    straightforward neural network architectures. Layers are added via the constructor\n    or through the `.add()` method.\n\n    Attributes:\n        layers (list): List of Layer instances in the model.\n\n    Example:\n        ```python\n        model = Sequential([\n            Dense(64, activation='relu', input_shape=(784,)),\n            Dense(10, activation='softmax')\n        ])\n        ```\n    \"\"\"\n\n    def __init__(self, layers):\n        \"\"\"\n        Initialize the Sequential model.\n\n        Args:\n            layers (list): Initial list of Layer instances to add to the model.\n        \"\"\"\n        initialize(self, layers)\n        initialize(self, layers)\n\n    add = add\n    build = build\n    copy = copy\n    compile = compile\n    fit = fit\n    evaluate = evaluate\n    predict = predict\n    save = save\n\n    @classmethod\n    def load(cls, filename):\n        \"\"\"\n        Load a model from a file.\n\n        Args:\n            filename (str): Path to the file containing the saved model.\n\n        Returns:\n            Sequential: Loaded model instance.\n\n        Example:\n            ```python\n            loaded_model = Sequential.load('my_model.json')\n            ```\n        \"\"\"\n        return load(cls, filename)\n</code></pre>"},{"location":"Api_Reference/Models/Sequential/Sequential_Class/#microkeras.models.Sequential-functions","title":"Functions","text":""},{"location":"Api_Reference/Models/Sequential/Sequential_Class/#microkeras.models.Sequential.__init__","title":"<code>__init__(layers)</code>","text":"<p>Initialize the Sequential model.</p> <p>Parameters:</p> Name Type Description Default <code>layers</code> <code>list</code> <p>Initial list of Layer instances to add to the model.</p> required Source code in <code>microkeras/models/sequential/sequential.py</code> <pre><code>def __init__(self, layers):\n    \"\"\"\n    Initialize the Sequential model.\n\n    Args:\n        layers (list): Initial list of Layer instances to add to the model.\n    \"\"\"\n    initialize(self, layers)\n    initialize(self, layers)\n</code></pre>"},{"location":"Api_Reference/Models/Sequential/Sequential_Class/#microkeras.models.Sequential.load","title":"<code>load(filename)</code>  <code>classmethod</code>","text":"<p>Load a model from a file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the file containing the saved model.</p> required <p>Returns:</p> Name Type Description <code>Sequential</code> <p>Loaded model instance.</p> Example <pre><code>loaded_model = Sequential.load('my_model.json')\n</code></pre> Source code in <code>microkeras/models/sequential/sequential.py</code> <pre><code>@classmethod\ndef load(cls, filename):\n    \"\"\"\n    Load a model from a file.\n\n    Args:\n        filename (str): Path to the file containing the saved model.\n\n    Returns:\n        Sequential: Loaded model instance.\n\n    Example:\n        ```python\n        loaded_model = Sequential.load('my_model.json')\n        ```\n    \"\"\"\n    return load(cls, filename)\n</code></pre>"},{"location":"Api_Reference/Operations/Backward/Backward_Layer_Method/","title":"Backward Layer Method","text":""},{"location":"Api_Reference/Operations/Backward/Backward_Layer_Method/#microkeras.operations.backward.backward_layer-functions","title":"Functions","text":""},{"location":"Api_Reference/Operations/Backward/Backward_Layer_Method/#microkeras.operations.backward.backward_layer.backward_layer","title":"<code>backward_layer(model, layer_index, X, Y, A_prev, loss, m)</code>","text":"<p>Perform backward propagation for a single layer in the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Sequential</code> <p>The neural network model.</p> required <code>layer_index</code> <code>int</code> <p>Index of the current layer.</p> required <code>X</code> <code>ndarray</code> <p>Input data.</p> required <code>Y</code> <code>ndarray</code> <p>True labels.</p> required <code>A_prev</code> <code>ndarray</code> <p>Activation from the previous layer.</p> required <code>loss</code> <code>str</code> <p>Loss function used.</p> required <code>m</code> <code>int</code> <p>Number of training examples.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>(dZ, dW, db) gradients for the current layer.</p> Example <pre><code>dZ, dW, db = backward_layer(model, 1, X, Y, A_prev, 'categorical_crossentropy', 32)\nprint(dZ.shape, dW.shape, db.shape)\n</code></pre> Source code in <code>microkeras/operations/backward/backward_layer.py</code> <pre><code>def backward_layer(model, layer_index, X, Y, A_prev, loss, m):\n    \"\"\"\n    Perform backward propagation for a single layer in the model.\n\n    Args:\n        model (Sequential): The neural network model.\n        layer_index (int): Index of the current layer.\n        X (numpy.ndarray): Input data.\n        Y (numpy.ndarray): True labels.\n        A_prev (numpy.ndarray): Activation from the previous layer.\n        loss (str): Loss function used.\n        m (int): Number of training examples.\n\n    Returns:\n        tuple: (dZ, dW, db) gradients for the current layer.\n\n    Example:\n        ```python\n        dZ, dW, db = backward_layer(model, 1, X, Y, A_prev, 'categorical_crossentropy', 32)\n        print(dZ.shape, dW.shape, db.shape)\n        ```\n    \"\"\"\n    layer = model.layers[layer_index]\n\n    dZ = calculate_dZ_wrapper(model, layer_index, Y, loss)\n    layer.dZ = dZ  # Set dZ for the current layer\n    dW = calculate_dW_wrapper(model, layer_index, X if layer_index == 0 else A_prev, m)\n    db = calculate_db_wrapper(model, layer_index, m)\n\n    return dZ, dW, db\n</code></pre>"},{"location":"Api_Reference/Operations/Backward/Backward_Method/","title":"Backward Method","text":""},{"location":"Api_Reference/Operations/Backward/Backward_Method/#microkeras.operations.backward.backward-functions","title":"Functions","text":""},{"location":"Api_Reference/Operations/Backward/Backward_Method/#microkeras.operations.backward.backward.backward","title":"<code>backward(model, X, Y, loss)</code>","text":"<p>Perform backward propagation through the entire model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Sequential</code> <p>The neural network model.</p> required <code>X</code> <code>ndarray</code> <p>Input data.</p> required <code>Y</code> <code>ndarray</code> <p>True labels.</p> required <code>loss</code> <code>str</code> <p>Loss function used.</p> required Example <pre><code>backward(model, X_train, Y_train, 'categorical_crossentropy')\n</code></pre> Note <p>This function updates the gradients (dZ, dW, db) for all layers in the model.</p> Source code in <code>microkeras/operations/backward/backward.py</code> <pre><code>def backward(model, X, Y, loss):\n    \"\"\"\n    Perform backward propagation through the entire model.\n\n    Args:\n        model (Sequential): The neural network model.\n        X (numpy.ndarray): Input data.\n        Y (numpy.ndarray): True labels.\n        loss (str): Loss function used.\n\n    Example:\n        ```python\n        backward(model, X_train, Y_train, 'categorical_crossentropy')\n        ```\n\n    Note:\n        This function updates the gradients (dZ, dW, db) for all layers in the model.\n    \"\"\"\n    m = X.shape[1]  # number of training examples\n    n_layers = len(model.layers)\n\n    for i in reversed(range(n_layers)):\n        if i == n_layers - 1:\n            A_prev = model.layers[i-1].A if i &gt; 0 else X\n        else:\n            A_prev = model.layers[i-1].A\n\n        dZ, dW, db = backward_layer(model, i, X, Y, A_prev, loss, m)\n\n        model.layers[i].dZ = dZ\n        model.layers[i].dW = dW\n        model.layers[i].db = db\n</code></pre>"},{"location":"Api_Reference/Operations/Backward/Calculate_DB_Method/","title":"Calculate DB Method","text":""},{"location":"Api_Reference/Operations/Backward/Calculate_DB_Method/#microkeras.operations.backward.calculate_db-functions","title":"Functions","text":""},{"location":"Api_Reference/Operations/Backward/Calculate_DB_Method/#microkeras.operations.backward.calculate_db.calculate_db","title":"<code>calculate_db(dZ, m)</code>","text":"<p>Calculate the gradient of the bias.</p> <p>Parameters:</p> Name Type Description Default <code>dZ</code> <code>ndarray</code> <p>Gradient of the cost with respect to the linear output.</p> required <code>m</code> <code>int</code> <p>Number of training examples.</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: Gradient of the bias.</p> Example <pre><code>db = calculate_db(dZ, 32)\nprint(db.shape)\n</code></pre> Source code in <code>microkeras/operations/backward/calculate_db.py</code> <pre><code>def calculate_db(dZ, m):\n    \"\"\"\n    Calculate the gradient of the bias.\n\n    Args:\n        dZ (numpy.ndarray): Gradient of the cost with respect to the linear output.\n        m (int): Number of training examples.\n\n    Returns:\n        numpy.ndarray: Gradient of the bias.\n\n    Example:\n        ```python\n        db = calculate_db(dZ, 32)\n        print(db.shape)\n        ```\n    \"\"\"\n    return (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n</code></pre>"},{"location":"Api_Reference/Operations/Backward/Calculate_DB_Wrapper_Method/","title":"Calculate DB Wrapper Method","text":""},{"location":"Api_Reference/Operations/Backward/Calculate_DB_Wrapper_Method/#microkeras.operations.backward.calculate_db_wrapper-functions","title":"Functions","text":""},{"location":"Api_Reference/Operations/Backward/Calculate_DB_Wrapper_Method/#microkeras.operations.backward.calculate_db_wrapper.calculate_db_wrapper","title":"<code>calculate_db_wrapper(model, i, m)</code>","text":"<p>Wrapper function to calculate the gradient of the bias for a specific layer.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Sequential</code> <p>The neural network model.</p> required <code>i</code> <code>int</code> <p>Index of the current layer.</p> required <code>m</code> <code>int</code> <p>Number of training examples.</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: Gradient of the bias for the specified layer.</p> Example <pre><code>db = calculate_db_wrapper(model, 1, 32)\nprint(db.shape)\n</code></pre> Source code in <code>microkeras/operations/backward/calculate_db_wrapper.py</code> <pre><code>def calculate_db_wrapper(model, i, m):\n    \"\"\"\n    Wrapper function to calculate the gradient of the bias for a specific layer.\n\n    Args:\n        model (Sequential): The neural network model.\n        i (int): Index of the current layer.\n        m (int): Number of training examples.\n\n    Returns:\n        numpy.ndarray: Gradient of the bias for the specified layer.\n\n    Example:\n        ```python\n        db = calculate_db_wrapper(model, 1, 32)\n        print(db.shape)\n        ```\n    \"\"\"\n    return calculate_db(model.layers[i].dZ, m)\n</code></pre>"},{"location":"Api_Reference/Operations/Backward/Calculate_DW_Method/","title":"Calculate DW Method","text":""},{"location":"Api_Reference/Operations/Backward/Calculate_DW_Method/#microkeras.operations.backward.calculate_dW-functions","title":"Functions","text":""},{"location":"Api_Reference/Operations/Backward/Calculate_DW_Method/#microkeras.operations.backward.calculate_dW.calculate_dW","title":"<code>calculate_dW(dZ, A_prev, m)</code>","text":"<p>Calculate the gradient of the weights.</p> <p>Parameters:</p> Name Type Description Default <code>dZ</code> <code>ndarray</code> <p>Gradient of the cost with respect to the linear output.</p> required <code>A_prev</code> <code>ndarray</code> <p>Activations from the previous layer.</p> required <code>m</code> <code>int</code> <p>Number of training examples.</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: Gradient of the weights.</p> Example <pre><code>dW = calculate_dW(dZ, A_prev, 32)\nprint(dW.shape)\n</code></pre> Source code in <code>microkeras/operations/backward/calculate_dW.py</code> <pre><code>def calculate_dW(dZ, A_prev, m):\n    \"\"\"\n    Calculate the gradient of the weights.\n\n    Args:\n        dZ (numpy.ndarray): Gradient of the cost with respect to the linear output.\n        A_prev (numpy.ndarray): Activations from the previous layer.\n        m (int): Number of training examples.\n\n    Returns:\n        numpy.ndarray: Gradient of the weights.\n\n    Example:\n        ```python\n        dW = calculate_dW(dZ, A_prev, 32)\n        print(dW.shape)\n        ```\n    \"\"\"\n    result = (1 / m) * np.dot(dZ, A_prev.T) \n    return np.array(result).astype(np.float64)\n</code></pre>"},{"location":"Api_Reference/Operations/Backward/Calculate_DW_Wrapper_Method/","title":"Calculate DW Wrapper Method","text":""},{"location":"Api_Reference/Operations/Backward/Calculate_DW_Wrapper_Method/#microkeras.operations.backward.calculate_dW_wrapper-functions","title":"Functions","text":""},{"location":"Api_Reference/Operations/Backward/Calculate_DW_Wrapper_Method/#microkeras.operations.backward.calculate_dW_wrapper.calculate_dW_wrapper","title":"<code>calculate_dW_wrapper(model, i, X, m)</code>","text":"<p>Wrapper function to calculate the gradient of the weights for a specific layer.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Sequential</code> <p>The neural network model.</p> required <code>i</code> <code>int</code> <p>Index of the current layer.</p> required <code>X</code> <code>ndarray</code> <p>Input data (used for the first layer).</p> required <code>m</code> <code>int</code> <p>Number of training examples.</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: Gradient of the weights for the specified layer.</p> Example <pre><code>dW = calculate_dW_wrapper(model, 1, X, 32)\nprint(dW.shape)\n</code></pre> Source code in <code>microkeras/operations/backward/calculate_dW_wrapper.py</code> <pre><code>def calculate_dW_wrapper(model, i, X, m):\n    \"\"\"\n    Wrapper function to calculate the gradient of the weights for a specific layer.\n\n    Args:\n        model (Sequential): The neural network model.\n        i (int): Index of the current layer.\n        X (numpy.ndarray): Input data (used for the first layer).\n        m (int): Number of training examples.\n\n    Returns:\n        numpy.ndarray: Gradient of the weights for the specified layer.\n\n    Example:\n        ```python\n        dW = calculate_dW_wrapper(model, 1, X, 32)\n        print(dW.shape)\n        ```\n    \"\"\"\n    current_layer = model.layers[i]\n\n    if i != 0:\n        previous_layer = model.layers[i-1]\n        return calculate_dW(current_layer.dZ, previous_layer.A, m)\n    else:\n        return calculate_dW(current_layer.dZ, X, m)\n</code></pre>"},{"location":"Api_Reference/Operations/Backward/Calculate_DZ_Linear_MSE_Method/","title":"Calculate DZ Linear MSE Method","text":""},{"location":"Api_Reference/Operations/Backward/Calculate_DZ_Linear_MSE_Method/#microkeras.operations.backward.calculate_dZ_linear_mean_squared_error-functions","title":"Functions","text":""},{"location":"Api_Reference/Operations/Backward/Calculate_DZ_Linear_MSE_Method/#microkeras.operations.backward.calculate_dZ_linear_mean_squared_error.calculate_dZ_linear_mean_squared_error","title":"<code>calculate_dZ_linear_mean_squared_error(A, Y, Z)</code>","text":"<p>Calculate dZ for linear activation with mean squared error loss.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>ndarray</code> <p>Activations of the current layer.</p> required <code>Y</code> <code>ndarray</code> <p>True labels.</p> required <code>Z</code> <code>ndarray</code> <p>Linear output of the current layer.</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: Gradient of Z.</p> Example <pre><code>dZ = calculate_dZ_linear_mean_squared_error(A, Y, Z)\nprint(dZ.shape)\n</code></pre> Source code in <code>microkeras/operations/backward/calculate_dZ_linear_mean_squared_error.py</code> <pre><code>def calculate_dZ_linear_mean_squared_error(A, Y, Z):\n    \"\"\"\n    Calculate dZ for linear activation with mean squared error loss.\n\n    Args:\n        A (numpy.ndarray): Activations of the current layer.\n        Y (numpy.ndarray): True labels.\n        Z (numpy.ndarray): Linear output of the current layer.\n\n    Returns:\n        numpy.ndarray: Gradient of Z.\n\n    Example:\n        ```python\n        dZ = calculate_dZ_linear_mean_squared_error(A, Y, Z)\n        print(dZ.shape)\n        ```\n    \"\"\"\n    m = A.shape[1]\n    dZ = 2 * (A - Y) / m\n    return dZ\n</code></pre>"},{"location":"Api_Reference/Operations/Backward/Calculate_DZ_Method/","title":"Calculate DZ Method","text":""},{"location":"Api_Reference/Operations/Backward/Calculate_DZ_Method/#microkeras.operations.backward.calculate_dZ-functions","title":"Functions","text":""},{"location":"Api_Reference/Operations/Backward/Calculate_DZ_Method/#microkeras.operations.backward.calculate_dZ.calculate_dZ","title":"<code>calculate_dZ(W_next, dZ_next, Z, activation)</code>","text":"<p>Calculate dZ for hidden layers.</p> <p>Parameters:</p> Name Type Description Default <code>W_next</code> <code>ndarray</code> <p>Weights of the next layer.</p> required <code>dZ_next</code> <code>ndarray</code> <p>dZ of the next layer.</p> required <code>Z</code> <code>ndarray</code> <p>Linear output of the current layer.</p> required <code>activation</code> <code>str</code> <p>Activation function of the current layer.</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: Gradient of Z.</p> Example <pre><code>dZ = calculate_dZ(W_next, dZ_next, Z, 'relu')\nprint(dZ.shape)\n</code></pre> Source code in <code>microkeras/operations/backward/calculate_dZ.py</code> <pre><code>def calculate_dZ(W_next, dZ_next, Z, activation):\n    \"\"\"\n    Calculate dZ for hidden layers.\n\n    Args:\n        W_next (numpy.ndarray): Weights of the next layer.\n        dZ_next (numpy.ndarray): dZ of the next layer.\n        Z (numpy.ndarray): Linear output of the current layer.\n        activation (str): Activation function of the current layer.\n\n    Returns:\n        numpy.ndarray: Gradient of Z.\n\n    Example:\n        ```python\n        dZ = calculate_dZ(W_next, dZ_next, Z, 'relu')\n        print(dZ.shape)\n        ```\n    \"\"\"\n    activation_derivatives = {\n        'sigmoid': sigmoid_derivative,\n        'relu': relu_derivative,\n        'linear': linear_derivative\n    }\n    activation_derivative = activation_derivatives[activation]\n    return np.dot(W_next.T, dZ_next) * activation_derivative(Z)\n</code></pre>"},{"location":"Api_Reference/Operations/Backward/Calculate_DZ_ReLU_MSE_Method/","title":"Calculate DZ ReLU MSE Method","text":""},{"location":"Api_Reference/Operations/Backward/Calculate_DZ_ReLU_MSE_Method/#microkeras.operations.backward.calculate_dZ_relu_mean_squared_error-functions","title":"Functions","text":""},{"location":"Api_Reference/Operations/Backward/Calculate_DZ_ReLU_MSE_Method/#microkeras.operations.backward.calculate_dZ_relu_mean_squared_error.calculate_dZ_relu_mean_squared_error","title":"<code>calculate_dZ_relu_mean_squared_error(A, Y, Z)</code>","text":"<p>Calculate dZ for ReLU activation with mean squared error loss.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>ndarray</code> <p>Activations of the current layer.</p> required <code>Y</code> <code>ndarray</code> <p>True labels.</p> required <code>Z</code> <code>ndarray</code> <p>Linear output of the current layer.</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: Gradient of Z.</p> Example <pre><code>dZ = calculate_dZ_relu_mean_squared_error(A, Y, Z)\nprint(dZ.shape)\n</code></pre> Source code in <code>microkeras/operations/backward/calculate_dZ_relu_mean_squared_error.py</code> <pre><code>def calculate_dZ_relu_mean_squared_error(A, Y, Z):\n    \"\"\"\n    Calculate dZ for ReLU activation with mean squared error loss.\n\n    Args:\n        A (numpy.ndarray): Activations of the current layer.\n        Y (numpy.ndarray): True labels.\n        Z (numpy.ndarray): Linear output of the current layer.\n\n    Returns:\n        numpy.ndarray: Gradient of Z.\n\n    Example:\n        ```python\n        dZ = calculate_dZ_relu_mean_squared_error(A, Y, Z)\n        print(dZ.shape)\n        ```\n    \"\"\"\n    m = A.shape[1]\n    dA = 2 * (A - Y) / m\n    dZ = dA * (Z &gt; 0)\n    return dZ\n</code></pre>"},{"location":"Api_Reference/Operations/Backward/Calculate_DZ_Softmax_CCE_Method/","title":"Calculate DZ Softmax CCE Method","text":""},{"location":"Api_Reference/Operations/Backward/Calculate_DZ_Softmax_CCE_Method/#microkeras.operations.backward.calculate_dZ_softmax_categorical_crossentropy-functions","title":"Functions","text":""},{"location":"Api_Reference/Operations/Backward/Calculate_DZ_Softmax_CCE_Method/#microkeras.operations.backward.calculate_dZ_softmax_categorical_crossentropy.calculate_dZ_softmax_categorical_crossentropy","title":"<code>calculate_dZ_softmax_categorical_crossentropy(A, Y)</code>","text":"<p>Calculate dZ for softmax activation with categorical cross-entropy loss.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>ndarray</code> <p>Activations of the current layer (softmax output).</p> required <code>Y</code> <code>ndarray</code> <p>True labels (one-hot encoded).</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: Gradient of Z.</p> Example <pre><code>dZ = calculate_dZ_softmax_categorical_crossentropy(A, Y)\nprint(dZ.shape)\n</code></pre> Source code in <code>microkeras/operations/backward/calculate_dZ_softmax_categorical_crossentropy.py</code> <pre><code>def calculate_dZ_softmax_categorical_crossentropy(A, Y):\n    \"\"\"\n    Calculate dZ for softmax activation with categorical cross-entropy loss.\n\n    Args:\n        A (numpy.ndarray): Activations of the current layer (softmax output).\n        Y (numpy.ndarray): True labels (one-hot encoded).\n\n    Returns:\n        numpy.ndarray: Gradient of Z.\n\n    Example:\n        ```python\n        dZ = calculate_dZ_softmax_categorical_crossentropy(A, Y)\n        print(dZ.shape)\n        ```\n    \"\"\"\n    return A - Y\n</code></pre>"},{"location":"Api_Reference/Operations/Backward/Calculate_DZ_Wrapper_Method/","title":"Calculate DZ Wrapper Method","text":""},{"location":"Api_Reference/Operations/Backward/Calculate_DZ_Wrapper_Method/#microkeras.operations.backward.calculate_dZ_wrapper-functions","title":"Functions","text":""},{"location":"Api_Reference/Operations/Backward/Calculate_DZ_Wrapper_Method/#microkeras.operations.backward.calculate_dZ_wrapper.calculate_dZ_wrapper","title":"<code>calculate_dZ_wrapper(model, i, Y, loss)</code>","text":"<p>Wrapper function to calculate dZ for the supported layer and loss combinations.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Sequential</code> <p>The neural network model.</p> required <code>i</code> <code>int</code> <p>Index of the current layer.</p> required <code>Y</code> <code>ndarray</code> <p>True labels.</p> required <code>loss</code> <code>str</code> <p>Loss function used.</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: Gradient of Z for the specified layer.</p> Example <pre><code>dZ = calculate_dZ_wrapper(model, 1, Y, 'categorical_crossentropy')\nprint(dZ.shape)\n</code></pre> Source code in <code>microkeras/operations/backward/calculate_dZ_wrapper.py</code> <pre><code>def calculate_dZ_wrapper(model, i, Y, loss):\n    \"\"\"\n    Wrapper function to calculate dZ for the supported layer and loss combinations.\n\n    Args:\n        model (Sequential): The neural network model.\n        i (int): Index of the current layer.\n        Y (numpy.ndarray): True labels.\n        loss (str): Loss function used.\n\n    Returns:\n        numpy.ndarray: Gradient of Z for the specified layer.\n\n    Example:\n        ```python\n        dZ = calculate_dZ_wrapper(model, 1, Y, 'categorical_crossentropy')\n        print(dZ.shape)\n        ```\n    \"\"\"\n    current_layer = model.layers[i]\n\n    if (i == len(model.layers) - 1):\n        if (current_layer.activation == 'softmax' and\n            loss == 'categorical_crossentropy'):\n            return calculate_dZ_softmax_categorical_crossentropy(current_layer.A, Y)\n        elif (current_layer.activation == 'relu' and\n              loss == 'mean_squared_error'):\n            return calculate_dZ_relu_mean_squared_error(current_layer.A, Y, current_layer.Z)\n        elif (current_layer.activation == 'linear' and\n              loss == 'mean_squared_error'):\n            return calculate_dZ_linear_mean_squared_error(current_layer.A, Y, current_layer.Z)\n\n    # General case\n    next_layer = model.layers[i + 1]\n    return calculate_dZ(next_layer.W,\n                        next_layer.dZ,\n                        current_layer.Z,\n                        current_layer.activation)\n</code></pre>"},{"location":"Api_Reference/Operations/Backward/Update_Params_Method/","title":"Update Params Method","text":""},{"location":"Api_Reference/Operations/Backward/Update_Params_Method/#microkeras.operations.backward.update_params-functions","title":"Functions","text":""},{"location":"Api_Reference/Operations/Backward/Update_Params_Method/#microkeras.operations.backward.update_params.update_params","title":"<code>update_params(model, learning_rate)</code>","text":"<p>Update the parameters (weights and biases) of all layers in the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Sequential</code> <p>The neural network model.</p> required <code>learning_rate</code> <code>float</code> <p>The learning rate for gradient descent.</p> required Example <pre><code>update_params(model, 0.01)\n</code></pre> Note <p>This function applies the computed gradients to update the model's parameters.</p> Source code in <code>microkeras/operations/backward/update_params.py</code> <pre><code>def update_params(model, learning_rate):\n    \"\"\"\n    Update the parameters (weights and biases) of all layers in the model.\n\n    Args:\n        model (Sequential): The neural network model.\n        learning_rate (float): The learning rate for gradient descent.\n\n    Example:\n        ```python\n        update_params(model, 0.01)\n        ```\n\n    Note:\n        This function applies the computed gradients to update the model's parameters.\n    \"\"\"\n    for layer in model.layers:\n        layer.W -= learning_rate * layer.dW\n        layer.b -= learning_rate * layer.db\n</code></pre>"},{"location":"Api_Reference/Operations/Forward/Calculate_A_Method/","title":"Calculate A Method","text":""},{"location":"Api_Reference/Operations/Forward/Calculate_A_Method/#microkeras.operations.forward.calculate_A-functions","title":"Functions","text":""},{"location":"Api_Reference/Operations/Forward/Calculate_A_Method/#microkeras.operations.forward.calculate_A.calculate_A","title":"<code>calculate_A(Z, activation)</code>","text":"<p>Calculate the activation output for a given input and activation function.</p> <p>Parameters:</p> Name Type Description Default <code>Z</code> <code>ndarray</code> <p>The input to the activation function.</p> required <code>activation</code> <code>str</code> <p>The name of the activation function to use.</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: The output after applying the activation function.</p> Example <pre><code>Z = np.array([[1, 2], [3, 4]])\nA = calculate_A(Z, 'sigmoid')\nprint(A.shape)\n</code></pre> Source code in <code>microkeras/operations/forward/calculate_A.py</code> <pre><code>def calculate_A(Z, activation):\n    \"\"\"\n    Calculate the activation output for a given input and activation function.\n\n    Args:\n        Z (numpy.ndarray): The input to the activation function.\n        activation (str): The name of the activation function to use.\n\n    Returns:\n        numpy.ndarray: The output after applying the activation function.\n\n    Example:\n        ```python\n        Z = np.array([[1, 2], [3, 4]])\n        A = calculate_A(Z, 'sigmoid')\n        print(A.shape)\n        ```\n    \"\"\"\n    if activation == 'sigmoid':\n        return sigmoid(Z)\n    elif activation == 'softmax':\n        return softmax(Z)\n    elif activation == 'relu':\n        return relu(Z)\n    elif activation == 'linear':\n        return linear(Z)\n</code></pre>"},{"location":"Api_Reference/Operations/Forward/Calculate_Z_Method/","title":"Calculate Z Method","text":""},{"location":"Api_Reference/Operations/Forward/Calculate_Z_Method/#microkeras.operations.forward.calculate_Z-functions","title":"Functions","text":""},{"location":"Api_Reference/Operations/Forward/Calculate_Z_Method/#microkeras.operations.forward.calculate_Z.calculate_Z","title":"<code>calculate_Z(W, A_prev, b)</code>","text":"<p>Calculate the linear combination Z = W * A_prev + b.</p> <p>Parameters:</p> Name Type Description Default <code>W</code> <code>ndarray</code> <p>Weight matrix of the current layer.</p> required <code>A_prev</code> <code>ndarray</code> <p>Activation output from the previous layer.</p> required <code>b</code> <code>ndarray</code> <p>Bias vector of the current layer.</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: The linear combination Z.</p> Example <pre><code>W = np.random.randn(3, 4)\nA_prev = np.random.randn(4, 5)\nb = np.random.randn(3, 1)\nZ = calculate_Z(W, A_prev, b)\nprint(Z.shape)\n</code></pre> Source code in <code>microkeras/operations/forward/calculate_Z.py</code> <pre><code>def calculate_Z(W, A_prev, b):\n    \"\"\"\n    Calculate the linear combination Z = W * A_prev + b.\n\n    Args:\n        W (numpy.ndarray): Weight matrix of the current layer.\n        A_prev (numpy.ndarray): Activation output from the previous layer.\n        b (numpy.ndarray): Bias vector of the current layer.\n\n    Returns:\n        numpy.ndarray: The linear combination Z.\n\n    Example:\n        ```python\n        W = np.random.randn(3, 4)\n        A_prev = np.random.randn(4, 5)\n        b = np.random.randn(3, 1)\n        Z = calculate_Z(W, A_prev, b)\n        print(Z.shape)\n        ```\n    \"\"\"\n    return np.dot(W, A_prev) + b\n</code></pre>"},{"location":"Api_Reference/Operations/Forward/Forward_Layer_Method/","title":"Forward Layer Method","text":""},{"location":"Api_Reference/Operations/Forward/Forward_Layer_Method/#microkeras.operations.forward.forward_layer-functions","title":"Functions","text":""},{"location":"Api_Reference/Operations/Forward/Forward_Layer_Method/#microkeras.operations.forward.forward_layer.forward_layer","title":"<code>forward_layer(layer, A_prev)</code>","text":"<p>Perform forward propagation for a single layer.</p> <p>Parameters:</p> Name Type Description Default <code>layer</code> <code>Layer</code> <p>The current layer object.</p> required <code>A_prev</code> <code>ndarray</code> <p>Activation output from the previous layer.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>(Z, A) Z (numpy.ndarray): The linear combination output. A (numpy.ndarray): The activation output.</p> Example <pre><code>layer = Dense(64, activation='relu')\nA_prev = np.random.randn(128, 32)\nZ, A = forward_layer(layer, A_prev)\nprint(Z.shape, A.shape)\n</code></pre> Source code in <code>microkeras/operations/forward/forward_layer.py</code> <pre><code>def forward_layer(layer, A_prev):\n    \"\"\"\n    Perform forward propagation for a single layer.\n\n    Args:\n        layer (Layer): The current layer object.\n        A_prev (numpy.ndarray): Activation output from the previous layer.\n\n    Returns:\n        tuple: (Z, A)\n            Z (numpy.ndarray): The linear combination output.\n            A (numpy.ndarray): The activation output.\n\n    Example:\n        ```python\n        layer = Dense(64, activation='relu')\n        A_prev = np.random.randn(128, 32)\n        Z, A = forward_layer(layer, A_prev)\n        print(Z.shape, A.shape)\n        ```\n    \"\"\"\n    Z = calculate_Z(layer.W, A_prev, layer.b)\n    A = calculate_A(Z, layer.activation)\n    return Z, A\n</code></pre>"},{"location":"Api_Reference/Operations/Forward/Forward_Method/","title":"Forward Method","text":""},{"location":"Api_Reference/Operations/Forward/Forward_Method/#microkeras.operations.forward.forward-functions","title":"Functions","text":""},{"location":"Api_Reference/Operations/Forward/Forward_Method/#microkeras.operations.forward.forward.forward","title":"<code>forward(nn, X)</code>","text":"<p>Perform forward propagation through the entire neural network.</p> <p>Parameters:</p> Name Type Description Default <code>nn</code> <code>Sequential</code> <p>The neural network model.</p> required <code>X</code> <code>ndarray</code> <p>The input data.</p> required <p>Returns:</p> Type Description <p>numpy.ndarray: The output of the last layer (final predictions).</p> Example <pre><code>model = Sequential([\n    Dense(64, activation='relu', input_shape=(128,)),\n    Dense(10, activation='softmax')\n])\nX = np.random.randn(128, 32)\noutput = forward(model, X)\nprint(output.shape)\n</code></pre> Note <p>This function updates the Z and A attributes of each layer in the network.</p> Source code in <code>microkeras/operations/forward/forward.py</code> <pre><code>def forward(nn, X):\n    \"\"\"\n    Perform forward propagation through the entire neural network.\n\n    Args:\n        nn (Sequential): The neural network model.\n        X (numpy.ndarray): The input data.\n\n    Returns:\n        numpy.ndarray: The output of the last layer (final predictions).\n\n    Example:\n        ```python\n        model = Sequential([\n            Dense(64, activation='relu', input_shape=(128,)),\n            Dense(10, activation='softmax')\n        ])\n        X = np.random.randn(128, 32)\n        output = forward(model, X)\n        print(output.shape)\n        ```\n\n    Note:\n        This function updates the Z and A attributes of each layer in the network.\n    \"\"\"\n    A = X\n    for layer in nn.layers:\n        Z, A = forward_layer(layer, A)\n        layer.Z = Z\n        layer.A = A\n    return A\n</code></pre>"},{"location":"Api_Reference/Operations/Metrics/Get_Accuracy_Method/","title":"Get Accuracy Method","text":""},{"location":"Api_Reference/Operations/Metrics/Get_Accuracy_Method/#microkeras.operations.metrics.get_accuracy-functions","title":"Functions","text":""},{"location":"Api_Reference/Operations/Metrics/Get_Accuracy_Method/#microkeras.operations.metrics.get_accuracy.get_accuracy","title":"<code>get_accuracy(model, X, Y)</code>","text":"<p>Calculate the accuracy of the model's predictions.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Sequential</code> <p>The neural network model.</p> required <code>X</code> <code>ndarray</code> <p>Input data, shape (n_features, n_samples).</p> required <code>Y</code> <code>ndarray</code> <p>True labels in one-hot encoded format, shape (n_classes, n_samples).</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The accuracy of the model's predictions, as a value between 0 and 1.</p> Example <pre><code>model = Sequential([\n    Dense(64, activation='relu', input_shape=(784,)),\n    Dense(10, activation='softmax')\n])\nX_test = np.random.randn(784, 100)\nY_test = np.eye(10)[np.random.randint(0, 10, 100)].T\naccuracy = get_accuracy(model, X_test, Y_test)\nprint(f\"Model accuracy: {accuracy}\")\n</code></pre> Note <ul> <li>This function assumes that the model's output and true labels are in one-hot encoded format.</li> <li>The accuracy is calculated as the ratio of correct predictions to total predictions.</li> </ul> Source code in <code>microkeras/operations/metrics/get_accuracy.py</code> <pre><code>def get_accuracy(model, X, Y):\n    \"\"\"\n    Calculate the accuracy of the model's predictions.\n\n    Args:\n        model (Sequential): The neural network model.\n        X (numpy.ndarray): Input data, shape (n_features, n_samples).\n        Y (numpy.ndarray): True labels in one-hot encoded format, shape (n_classes, n_samples).\n\n    Returns:\n        float: The accuracy of the model's predictions, as a value between 0 and 1.\n\n    Example:\n        ```python\n        model = Sequential([\n            Dense(64, activation='relu', input_shape=(784,)),\n            Dense(10, activation='softmax')\n        ])\n        X_test = np.random.randn(784, 100)\n        Y_test = np.eye(10)[np.random.randint(0, 10, 100)].T\n        accuracy = get_accuracy(model, X_test, Y_test)\n        print(f\"Model accuracy: {accuracy}\")\n        ```\n\n    Note:\n        - This function assumes that the model's output and true labels are in one-hot encoded format.\n        - The accuracy is calculated as the ratio of correct predictions to total predictions.\n    \"\"\"\n    forward(model, X)\n    predictions = np.argmax(model.layers[-1].A, axis=0)\n    Y_decoded = np.argmax(Y, axis=0)\n    return np.sum(predictions == Y_decoded) / Y_decoded.size\n</code></pre>"},{"location":"Api_Reference/Optimizers/Gradient_Descent_Method/","title":"Gradient Descent Method","text":""},{"location":"Api_Reference/Optimizers/Gradient_Descent_Method/#microkeras.optimizers.gradient_descent-functions","title":"Functions","text":""},{"location":"Api_Reference/Optimizers/Gradient_Descent_Method/#microkeras.optimizers.gradient_descent.gradient_descent","title":"<code>gradient_descent(model, X_train, Y_train, loss, learning_rate)</code>","text":"<p>Perform one step of gradient descent on the model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Sequential</code> <p>The neural network model.</p> required <code>X_train</code> <code>ndarray</code> <p>Input training data.</p> required <code>Y_train</code> <code>ndarray</code> <p>True labels for training data.</p> required <code>loss</code> <code>str</code> <p>The loss function to use.</p> required <code>learning_rate</code> <code>float</code> <p>The learning rate for parameter updates.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>(accuracy, loss_value) accuracy (float): The model's accuracy on the training data. loss_value (float): The loss value for the current state of the model.</p> Example <pre><code>acc, loss = gradient_descent(model, X_batch, Y_batch, 'categorical_crossentropy', 0.01)\nprint(f\"Accuracy: {acc}, Loss: {loss}\")\n</code></pre> Note <p>This function performs forward propagation, backward propagation, and parameter updates. It also calculates and returns the accuracy and loss for the current state of the model.</p> Source code in <code>microkeras/optimizers/gradient_descent.py</code> <pre><code>def gradient_descent(model, X_train, Y_train, loss, learning_rate):\n    \"\"\"\n    Perform one step of gradient descent on the model.\n\n    Args:\n        model (Sequential): The neural network model.\n        X_train (numpy.ndarray): Input training data.\n        Y_train (numpy.ndarray): True labels for training data.\n        loss (str): The loss function to use.\n        learning_rate (float): The learning rate for parameter updates.\n\n    Returns:\n        tuple: (accuracy, loss_value)\n            accuracy (float): The model's accuracy on the training data.\n            loss_value (float): The loss value for the current state of the model.\n\n    Example:\n        ```python\n        acc, loss = gradient_descent(model, X_batch, Y_batch, 'categorical_crossentropy', 0.01)\n        print(f\"Accuracy: {acc}, Loss: {loss}\")\n        ```\n\n    Note:\n        This function performs forward propagation, backward propagation, and parameter updates.\n        It also calculates and returns the accuracy and loss for the current state of the model.\n    \"\"\"\n    forward(model, X_train)\n    backward(model, X_train, Y_train, loss)\n    update_params(model, learning_rate)\n\n    # Calculate accuracy\n    acc = get_accuracy(model, X_train, Y_train)\n\n    # Calculate loss\n    if loss == 'categorical_crossentropy':\n        loss_val = categorical_crossentropy(Y_train, model.layers[-1].A)\n    elif loss == 'mean_squared_error':\n        loss_val = mean_squared_error(Y_train, model.layers[-1].A)\n\n    return acc, loss_val\n</code></pre>"},{"location":"Api_Reference/Optimizers/Initialize_Method/","title":"Initialize Method","text":""},{"location":"Api_Reference/Optimizers/Initialize_Method/#microkeras.optimizers.initialize-functions","title":"Functions","text":""},{"location":"Api_Reference/Optimizers/Initialize_Method/#microkeras.optimizers.initialize.initialize","title":"<code>initialize(self, learning_rate)</code>","text":"<p>Initialize the optimizer with a learning rate.</p> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>float</code> <p>The learning rate to use for parameter updates.</p> required Example <pre><code>optimizer = SGD()\noptimizer.initialize(0.01)\n</code></pre> Source code in <code>microkeras/optimizers/initialize.py</code> <pre><code>def initialize(self, learning_rate):\n    \"\"\"\n    Initialize the optimizer with a learning rate.\n\n    Args:\n        learning_rate (float): The learning rate to use for parameter updates.\n\n    Example:\n        ```python\n        optimizer = SGD()\n        optimizer.initialize(0.01)\n        ```\n    \"\"\"\n    self.learning_rate = learning_rate\n</code></pre>"},{"location":"Api_Reference/Optimizers/Minimize_Method/","title":"Minimize Method","text":""},{"location":"Api_Reference/Optimizers/Minimize_Method/#microkeras.optimizers.minimize-functions","title":"Functions","text":""},{"location":"Api_Reference/Optimizers/Minimize_Method/#microkeras.optimizers.minimize.minimize","title":"<code>minimize(self, model, X_train, Y_train, loss, batch_size, metrics)</code>","text":"<p>Minimize the loss function using mini-batch gradient descent.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Sequential</code> <p>The neural network model.</p> required <code>X_train</code> <code>ndarray</code> <p>Input training data.</p> required <code>Y_train</code> <code>ndarray</code> <p>True labels for training data.</p> required <code>loss</code> <code>str</code> <p>The loss function to use.</p> required <code>batch_size</code> <code>int</code> <p>The size of each mini-batch.</p> required <code>metrics</code> <code>list</code> <p>List of metrics to compute during training.</p> required Example <pre><code>optimizer = SGD(learning_rate=0.01)\noptimizer.minimize(model, X_train, Y_train, 'categorical_crossentropy', 32, ['accuracy'])\n</code></pre> Note <p>This function performs mini-batch gradient descent, updating the model parameters to minimize the loss function. It uses a progress bar to show training progress and computes specified metrics.</p> Source code in <code>microkeras/optimizers/minimize.py</code> <pre><code>def minimize(self, model, X_train, Y_train, loss, batch_size, metrics):\n    \"\"\"\n    Minimize the loss function using mini-batch gradient descent.\n\n    Args:\n        model (Sequential): The neural network model.\n        X_train (numpy.ndarray): Input training data.\n        Y_train (numpy.ndarray): True labels for training data.\n        loss (str): The loss function to use.\n        batch_size (int): The size of each mini-batch.\n        metrics (list): List of metrics to compute during training.\n\n    Example:\n        ```python\n        optimizer = SGD(learning_rate=0.01)\n        optimizer.minimize(model, X_train, Y_train, 'categorical_crossentropy', 32, ['accuracy'])\n        ```\n\n    Note:\n        This function performs mini-batch gradient descent, updating the model parameters\n        to minimize the loss function. It uses a progress bar to show training progress\n        and computes specified metrics.\n    \"\"\"\n    # Number of training examples\n    m = X_train.shape[1]\n    # Number of batches per epoch\n    num_iterations = m // batch_size  # Number of batches per epoch\n    if m % batch_size != 0:\n        # Ensure all data is used\n        num_iterations += 1\n\n    update_frequency = max(num_iterations // 30, 1)\n\n    # Create tqdm progress bar\n    pbar = tqdm(total=num_iterations, desc=\"SGD Progress\")\n\n    loss_list = []\n    accuracy_list = [] if 'accuracy' in metrics else None\n\n    for i in range(num_iterations):\n        X_batch, Y_batch = select_batches(X_train, Y_train, batch_size)\n        gradient_descent(model, X_batch, Y_batch, loss, self.learning_rate)\n\n        # Update the progress bar at the specified frequency\n        if i % update_frequency == 0 or i == num_iterations - 1:\n            # Calculate loss for the batch\n            forward(model, X_batch)\n            if loss == 'categorical_crossentropy':\n                loss_value = categorical_crossentropy(Y_batch, model.layers[-1].A)\n            elif loss == 'mean_squared_error':\n                loss_value = mean_squared_error(Y_batch, model.layers[-1].A)\n\n            loss_list.append(loss_value)\n\n            # Calculate accuracy for the batch if it's in metrics\n            if 'accuracy' in metrics:\n                acc = get_accuracy(model, X_batch, Y_batch)\n                accuracy_list.append(acc)\n\n            average_loss = sum(loss_list) / len(loss_list)\n            desc = f\"Batch {i+1}/{num_iterations} - Loss: {average_loss:.4f}\"\n\n            if 'accuracy' in metrics:\n                average_acc = sum(accuracy_list) / len(accuracy_list)\n                desc += f\", Acc: {average_acc:.4f}\"\n\n            pbar.set_description(desc)\n            pbar.update(update_frequency)\n\n    pbar.close()\n</code></pre>"},{"location":"Api_Reference/Optimizers/Minimize_Wrapper_Method/","title":"Minimize Wrapper Method","text":""},{"location":"Api_Reference/Optimizers/Minimize_Wrapper_Method/#microkeras.optimizers.minimize_wrapper-functions","title":"Functions","text":""},{"location":"Api_Reference/Optimizers/Minimize_Wrapper_Method/#microkeras.optimizers.minimize_wrapper.minimize_wrapper","title":"<code>minimize_wrapper(optimizer, model, X_train, Y_train, loss, batch_size, epochs, metrics)</code>","text":"<p>A wrapper function to perform multiple epochs of training using the minimize function.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <p>The optimizer instance.</p> required <code>model</code> <code>Sequential</code> <p>The neural network model.</p> required <code>X_train</code> <code>ndarray</code> <p>Input training data.</p> required <code>Y_train</code> <code>ndarray</code> <p>True labels for training data.</p> required <code>loss</code> <code>str</code> <p>The loss function to use.</p> required <code>batch_size</code> <code>int</code> <p>The size of each mini-batch.</p> required <code>epochs</code> <code>int</code> <p>The number of epochs to train for.</p> required <code>metrics</code> <code>list</code> <p>List of metrics to compute during training.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A history dictionary containing the loss and specified metrics for each epoch.</p> Example <pre><code>history = minimize_wrapper(optimizer, model, X_train, Y_train, 'categorical_crossentropy', 32, 10, ['accuracy'])\nprint(history['loss'])\nprint(history['accuracy'])\n</code></pre> Note <p>This function manages the training process over multiple epochs, calling the minimize function for each epoch and collecting the training history.</p> Source code in <code>microkeras/optimizers/minimize_wrapper.py</code> <pre><code>def minimize_wrapper(optimizer, model, X_train, Y_train, loss, batch_size, epochs, metrics):\n    \"\"\"\n    A wrapper function to perform multiple epochs of training using the minimize function.\n\n    Args:\n        optimizer: The optimizer instance.\n        model (Sequential): The neural network model.\n        X_train (numpy.ndarray): Input training data.\n        Y_train (numpy.ndarray): True labels for training data.\n        loss (str): The loss function to use.\n        batch_size (int): The size of each mini-batch.\n        epochs (int): The number of epochs to train for.\n        metrics (list): List of metrics to compute during training.\n\n    Returns:\n        dict: A history dictionary containing the loss and specified metrics for each epoch.\n\n    Example:\n        ```python\n        history = minimize_wrapper(optimizer, model, X_train, Y_train, 'categorical_crossentropy', 32, 10, ['accuracy'])\n        print(history['loss'])\n        print(history['accuracy'])\n        ```\n\n    Note:\n        This function manages the training process over multiple epochs, calling the minimize\n        function for each epoch and collecting the training history.\n    \"\"\"\n    history = {}\n\n    for metric in metrics:\n        if metric in ['accuracy']:\n            history[metric] = []\n    history['loss'] = []\n\n    for epoch in range(epochs):\n        print(f\"Epoch {epoch + 1}/{epochs}\")\n        minimize(optimizer, model, X_train, Y_train, loss, batch_size, metrics)\n\n        # Calculate metrics for the dataset after each epoch\n        forward(model, X_train)  # Update the forward pass for entire dataset\n\n        if 'accuracy' in metrics:\n            acc = get_accuracy(model, X_train, Y_train)\n            history['accuracy'].append(acc)\n\n        if loss == 'categorical_crossentropy':\n            loss_value = categorical_crossentropy(Y_train, model.layers[-1].A)\n        elif loss == 'mean_squared_error':\n            loss_value = mean_squared_error(Y_train, model.layers[-1].A)\n\n        history['loss'].append(loss_value)\n\n    return history\n</code></pre>"},{"location":"Api_Reference/Optimizers/SGD_Class/","title":"SGD Class","text":"<p>Stochastic Gradient Descent (SGD) optimizer.</p> <p>This class implements the SGD optimization algorithm.</p> <p>Attributes:</p> Name Type Description <code>learning_rate</code> <code>float</code> <p>The learning rate for parameter updates.</p> <p>Methods:</p> Name Description <code>minimize_wrapper</code> <p>A method to perform training over multiple epochs.</p> Example <pre><code>optimizer = SGD(learning_rate=0.01)\nhistory = optimizer.minimize_wrapper(model, X_train, Y_train, 'categorical_crossentropy', 32, 10, ['accuracy'])\n</code></pre> Source code in <code>microkeras/optimizers/sgd.py</code> <pre><code>class SGD:\n    \"\"\"\n    Stochastic Gradient Descent (SGD) optimizer.\n\n    This class implements the SGD optimization algorithm.\n\n    Attributes:\n        learning_rate (float): The learning rate for parameter updates.\n\n    Methods:\n        minimize_wrapper: A method to perform training over multiple epochs.\n\n    Example:\n        ```python\n        optimizer = SGD(learning_rate=0.01)\n        history = optimizer.minimize_wrapper(model, X_train, Y_train, 'categorical_crossentropy', 32, 10, ['accuracy'])\n        ```\n    \"\"\"\n    def __init__(self, learning_rate=0.01):\n        \"\"\"\n        Initialize the SGD optimizer.\n\n        Args:\n            learning_rate (float): The learning rate to use for parameter updates. Default is 0.01.\n        \"\"\"\n        self.learning_rate = learning_rate\n        self.learning_rate = learning_rate\n\n    minimize_wrapper = minimize_wrapper\n</code></pre>"},{"location":"Api_Reference/Optimizers/SGD_Class/#microkeras.optimizers.SGD-functions","title":"Functions","text":""},{"location":"Api_Reference/Optimizers/SGD_Class/#microkeras.optimizers.SGD.__init__","title":"<code>__init__(learning_rate=0.01)</code>","text":"<p>Initialize the SGD optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>learning_rate</code> <code>float</code> <p>The learning rate to use for parameter updates. Default is 0.01.</p> <code>0.01</code> Source code in <code>microkeras/optimizers/sgd.py</code> <pre><code>def __init__(self, learning_rate=0.01):\n    \"\"\"\n    Initialize the SGD optimizer.\n\n    Args:\n        learning_rate (float): The learning rate to use for parameter updates. Default is 0.01.\n    \"\"\"\n    self.learning_rate = learning_rate\n    self.learning_rate = learning_rate\n</code></pre>"},{"location":"Api_Reference/Optimizers/Select_Batch_Method/","title":"Select Batch Method","text":"<p>Randomly select a batch of samples from the training data.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>ndarray</code> <p>Input training data.</p> required <code>Y_train</code> <code>ndarray</code> <p>True labels for training data.</p> required <code>batch_size</code> <code>int</code> <p>The size of the batch to select.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>(X_batch, Y_batch) X_batch (numpy.ndarray): The selected batch of input data. Y_batch (numpy.ndarray): The corresponding batch of labels.</p> Example <pre><code>X_batch, Y_batch = select_batches(X_train, Y_train, 32)\nprint(X_batch.shape, Y_batch.shape)\n</code></pre> Note <p>This function handles both 1D and 2D label arrays (Y_train).</p> Source code in <code>microkeras/optimizers/select_batch.py</code> <pre><code>def select_batches(X_train, Y_train, batch_size):\n    \"\"\"\n    Randomly select a batch of samples from the training data.\n\n    Args:\n        X_train (numpy.ndarray): Input training data.\n        Y_train (numpy.ndarray): True labels for training data.\n        batch_size (int): The size of the batch to select.\n\n    Returns:\n        tuple: (X_batch, Y_batch)\n            X_batch (numpy.ndarray): The selected batch of input data.\n            Y_batch (numpy.ndarray): The corresponding batch of labels.\n\n    Example:\n        ```python\n        X_batch, Y_batch = select_batches(X_train, Y_train, 32)\n        print(X_batch.shape, Y_batch.shape)\n        ```\n\n    Note:\n        This function handles both 1D and 2D label arrays (Y_train).\n    \"\"\"\n    m = X_train.shape[1]  # number of training examples\n    batch_indices = np.random.choice(m, batch_size, replace=False)\n    X_batch = X_train[:, batch_indices]\n\n    # Check if Y_train is 1D or 2D and select accordingly\n    if Y_train.ndim == 1:\n        Y_batch = Y_train[batch_indices]\n    else:\n        Y_batch = Y_train[:, batch_indices]\n\n    return X_batch, Y_batch\n</code></pre>"},{"location":"Examples/California_Housing_Regression/","title":"California Housing Regression Example","text":"<p>This example demonstrates how to use MicroKeras for a regression task using the California Housing dataset. We'll build a model to predict housing prices based on various features.</p>"},{"location":"Examples/California_Housing_Regression/#importing-dependencies","title":"Importing Dependencies","text":"<p>First, let's import the necessary modules:</p> <pre><code>import numpy as np\nfrom microkeras.models import Sequential\nfrom microkeras.layers import Dense\nfrom microkeras.optimizers import SGD\nfrom microkeras.datasets import california_housing\n</code></pre>"},{"location":"Examples/California_Housing_Regression/#loading-and-preprocessing-data","title":"Loading and Preprocessing Data","text":"<p>Next, we'll load the California Housing dataset and preprocess it:</p> <pre><code># Load and preprocess California Housing data\n(X_train, y_train), (X_test, y_test) = california_housing.load_data()\n\n# Reshape y to be 2D\ny_train = y_train.reshape(-1, 1)\ny_test = y_test.reshape(-1, 1)\n</code></pre>"},{"location":"Examples/California_Housing_Regression/#creating-the-model","title":"Creating the Model","text":"<p>Now, let's create our Sequential model:</p> <pre><code>model = Sequential([\n    Dense(128, activation='relu', input_shape=(8,)),\n    Dense(64, activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(1, activation='linear')\n])\n</code></pre> <p>This model consists of three hidden layers with ReLU activation and an output layer with linear activation, suitable for regression tasks.</p>"},{"location":"Examples/California_Housing_Regression/#compiling-the-model","title":"Compiling the Model","text":"<p>We'll compile the model using Stochastic Gradient Descent (SGD) as the optimizer and Mean Squared Error (MSE) as the loss function:</p> <pre><code>optimizer = SGD(learning_rate=0.01)\nmodel.compile(optimizer=optimizer,\n              loss='mean_squared_error',\n              metrics=[])  # Empty list for metrics as it's a regression problem\n</code></pre>"},{"location":"Examples/California_Housing_Regression/#training-the-model","title":"Training the Model","text":"<p>Let's train the model for 100 epochs with a batch size of 16:</p> <pre><code>history = model.fit(X_train,\n                    y_train,\n                    batch_size=16,\n                    epochs=100)\n</code></pre>"},{"location":"Examples/California_Housing_Regression/#evaluating-the-model","title":"Evaluating the Model","text":"<p>After training, we can evaluate the model on the test set:</p> <pre><code>test_mse = model.evaluate(X_test, y_test)\nprint(f\"Test MSE: {test_mse:.4f}\")\n</code></pre>"},{"location":"Examples/California_Housing_Regression/#making-predictions","title":"Making Predictions","text":"<p>Let's make predictions for the first 5 test samples:</p> <pre><code>predictions = model.predict(X_test[:5])\nprint(\"Predictions for the first 5 test samples:\")\nprint(predictions.flatten())\nprint(\"Actual values:\")\nprint(y_test[:5].flatten())\n</code></pre>"},{"location":"Examples/California_Housing_Regression/#saving-and-loading-the-model","title":"Saving and Loading the Model","text":"<p>MicroKeras allows you to save and load models:</p> <pre><code># Save the model\nmodel.save('example_models/california_housing_model.json')\n\n# Load the model\nloaded_model = Sequential.load('example_models/california_housing_model.json')\n\n# Compile the loaded model\nloaded_model.compile(optimizer=optimizer,\n                     loss='mean_squared_error',\n                     metrics=[])\n\n# Evaluate the loaded model\nloaded_test_mse = loaded_model.evaluate(X_test, y_test)\nprint(f\"Loaded model test MSE: {loaded_test_mse:.4f}\")\n</code></pre>"},{"location":"Examples/California_Housing_Regression/#viewing-training-history","title":"Viewing Training History","text":"<p>Finally, let's print out the training history:</p> <pre><code>print(\"\\nTraining History:\")\nprint(\"Epoch\\tLoss\")\nfor epoch, loss in enumerate(history['loss'], 1):\n    print(f\"{epoch}\\t{loss:.4f}\")\n</code></pre> <p>This example demonstrates how to use MicroKeras for a regression task, including model creation, training, evaluation, prediction, and model saving/loading.</p>"},{"location":"Examples/MNIST_Classification/","title":"MNIST Classification Example","text":"<p>Let's walk through a complete example using MicroKeras to classify handwritten digits from the MNIST dataset. This example will demonstrate how to load data, create a model, train it, make predictions, and save/load the model.</p>"},{"location":"Examples/MNIST_Classification/#importing-dependencies","title":"Importing Dependencies","text":"<p>First, let's import the necessary modules:</p> <pre><code>import numpy as np\nfrom microkeras.models import Sequential\nfrom microkeras.layers import Dense\nfrom microkeras.optimizers import SGD\nfrom microkeras.datasets import mnist\n</code></pre>"},{"location":"Examples/MNIST_Classification/#loading-and-preprocessing-data","title":"Loading and Preprocessing Data","text":"<p>Next, we'll load the MNIST dataset and preprocess it:</p> <pre><code># Load and preprocess MNIST data\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n# Reshape and normalize the input data\nX_train = X_train.reshape(X_train.shape[0], -1)\nX_test = X_test.reshape(X_test.shape[0], -1)\n\n# One-hot encode the labels\ny_train = np.eye(10)[y_train]\ny_test = np.eye(10)[y_test]\n\nprint(\"Data shapes:\")\nprint(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\nprint(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n</code></pre> <p>Output:</p> <pre><code>Data shapes:\nX_train: (56000, 784), y_train: (56000, 10)\nX_test: (14000, 784), y_test: (14000, 10)\n</code></pre>"},{"location":"Examples/MNIST_Classification/#creating-the-model","title":"Creating the Model","text":"<p>Now, let's create our Sequential model:</p> <pre><code>model = Sequential([\n    Dense(200, activation='sigmoid', input_shape=(784,)),\n    Dense(200, activation='sigmoid'),\n    Dense(10, activation='softmax')\n])\n</code></pre> <p>This model consists of two hidden layers with sigmoid activation and an output layer with softmax activation, suitable for multi-class classification.</p>"},{"location":"Examples/MNIST_Classification/#compiling-the-model","title":"Compiling the Model","text":"<p>We'll compile the model using Stochastic Gradient Descent (SGD) as the optimizer and Categorical Cross-Entropy as the loss function:</p> <pre><code>optimizer = SGD(learning_rate=0.1)\nmodel.compile(optimizer=optimizer,\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n</code></pre>"},{"location":"Examples/MNIST_Classification/#training-the-model","title":"Training the Model","text":"<p>Let's train the model for 10 epochs with a batch size of 32:</p> <pre><code>history = model.fit(X_train,\n                    y_train,\n                    batch_size=16,\n                    epochs=10)\n</code></pre> <p>Output:</p> <pre><code>Epoch 1/10\nBatch 3500/3500 - Loss: 4.9410, Acc: 0.9238: : 3712it [00:37, 99.54it/s] \nEpoch 2/10\nBatch 3500/3500 - Loss: 1.9095, Acc: 0.9746: : 3712it [00:20, 180.73it/s]\nEpoch 3/10\nBatch 3500/3500 - Loss: 1.5755, Acc: 0.9844: : 3712it [00:21, 168.76it/s]\nEpoch 4/10\nBatch 3500/3500 - Loss: 1.2145, Acc: 0.9844: : 3712it [00:22, 162.42it/s]\nEpoch 5/10\nBatch 3500/3500 - Loss: 0.9108, Acc: 0.9902: : 3712it [00:23, 157.83it/s]\nEpoch 6/10\nBatch 3500/3500 - Loss: 0.7050, Acc: 0.9941: : 3712it [00:21, 175.94it/s]\nEpoch 7/10\nBatch 3500/3500 - Loss: 0.4757, Acc: 0.9980: : 3712it [00:20, 183.64it/s]\nEpoch 8/10\nBatch 3500/3500 - Loss: 0.4843, Acc: 1.0000: : 3712it [00:22, 164.84it/s]\nEpoch 9/10\nBatch 3500/3500 - Loss: 0.4095, Acc: 0.9980: : 3712it [00:22, 164.12it/s]\nEpoch 10/10\nBatch 3500/3500 - Loss: 0.3842, Acc: 0.9961: : 3712it [00:21, 176.15it/s]\n</code></pre>"},{"location":"Examples/MNIST_Classification/#plotting-training-history","title":"Plotting Training History","text":"<p>We can plot the training history using Matplotlib:</p> <pre><code># Plot training history\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(history['accuracy'], label='Training Accuracy')\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history['loss'], label='Training Loss')\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Output:</p> <p></p>"},{"location":"Examples/MNIST_Classification/#evaluating-the-model","title":"Evaluating the Model","text":"<p>After training, we can evaluate the model on the test set:</p> <pre><code>test_loss, test_accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test accuracy: {test_accuracy:.4f}\")\n</code></pre> <p>Output:</p> <pre><code>Test accuracy: 0.9671\n</code></pre>"},{"location":"Examples/MNIST_Classification/#making-predictions","title":"Making Predictions","text":"<p>Let's make predictions for the first 5 test samples:</p> <pre><code>predictions = model.predict(X_test[:5])\nprint(\"Predictions for the first 5 test samples:\")\nprint(np.argmax(predictions, axis=1))\nprint(\"Actual labels:\")\nprint(np.argmax(y_test[:5], axis=1))\n\n# Visualize the predictions\nplt.figure(figsize=(12, 4))\nfor i in range(5):\n    plt.subplot(1, 5, i+1)\n    plt.imshow(X_test[i].reshape(28, 28), cmap='gray')\n    plt.title(f\"Pred: {np.argmax(predictions[i])}\\nTrue: {np.argmax(y_test[i])}\")\n    plt.axis('off')\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Output:</p> <pre><code>Predictions for the first 5 test samples:\n[8 4 8 7 7]\nActual labels:\n[8 4 8 7 7]\n</code></pre> <p></p>"},{"location":"Examples/MNIST_Classification/#saving-and-loading-the-model","title":"Saving and Loading the Model","text":"<p>MicroKeras allows you to save and load models:</p> <pre><code># Save the model\nmodel.save('mnist_model.json')\n\n# Load the model\nloaded_model = Sequential.load('mnist_model.json')\n\n# Compile the loaded model\nloaded_model.compile(optimizer=optimizer,\n                     loss='categorical_crossentropy',\n                     metrics=['accuracy'])\n\n# Evaluate the loaded model\nloaded_test_loss, loaded_test_accuracy = loaded_model.evaluate(X_test, y_test)\nprint(f\"Loaded model test accuracy: {loaded_test_accuracy:.4f}\")\n</code></pre> <p>Output:</p> <pre><code>Loaded model test accuracy: 0.9671\n</code></pre>"},{"location":"Examples/MNIST_Classification/#viewing-training-history","title":"Viewing Training History","text":"<p>Finally, let's print out the training history:</p> <pre><code>print(\"\\nTraining History:\")\nprint(\"Epoch\\tAccuracy\\tLoss\")\nfor epoch, (accuracy, loss) in enumerate(zip(history['accuracy'], history['loss']), 1):\n    print(f\"{epoch}\\t{accuracy:.4f}\\t{loss:.4f}\")\n</code></pre> <p>Output:</p> <pre><code>Training History:\nEpoch   Accuracy    Loss\n1   0.9235  14096.9166\n2   0.9467  9728.8764\n3   0.9592  7629.1049\n4   0.9683  6160.9156\n5   0.9730  5208.7316\n6   0.9761  4604.9854\n7   0.9803  3969.1862\n8   0.9828  3443.3674\n9   0.9831  3310.2258\n10  0.9851  3007.7300\n</code></pre>"},{"location":"Testing/Running_Tests/","title":"Running Tests","text":"<p>To ensure the library is functioning correctly, you can run the included tests. This guide will show you how to run all tests or specific tests for MicroKeras.</p>"},{"location":"Testing/Running_Tests/#prerequisites","title":"Prerequisites","text":"<p>Before running the tests, make sure you have installed MicroKeras and its dependencies, including pytest. If you haven't installed pytest, you can do so using pip:</p> <pre><code>pip install pytest\n</code></pre>"},{"location":"Testing/Running_Tests/#running-all-tests","title":"Running All Tests","text":"<p>To run all tests for MicroKeras, use the following command from the root directory of the project:</p> <pre><code>pytest -v -s tests\n</code></pre> <p>This command will run all test files in the <code>tests</code> directory and its subdirectories.</p>"},{"location":"Testing/Running_Tests/#running-specific-tests","title":"Running Specific Tests","text":"<p>If you want to run a specific test file, you can specify the path to that file. For example, to run the tests for the sigmoid activation function:</p> <pre><code>pytest -v -s tests/test_sigmoid.py\n</code></pre> <p>Replace <code>test_sigmoid.py</code> with the name of the specific test file you want to run.</p>"}]}